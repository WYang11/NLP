





# æœºå™¨ç¿»è¯‘

æœºå™¨ç¿»è¯‘æ˜¯æŒ‡å°†ä¸€æ®µæ–‡æœ¬ä»ä¸€ç§è¯­è¨€è‡ªåŠ¨ç¿»è¯‘åˆ°å¦ä¸€ç§è¯­è¨€ã€‚å› ä¸ºä¸€æ®µæ–‡æœ¬åºåˆ—åœ¨ä¸åŒè¯­è¨€ä¸­çš„é•¿åº¦ä¸ä¸€å®šç›¸åŒï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨æœºå™¨ç¿»è¯‘ä¸ºä¾‹æ¥ä»‹ç»ç¼–ç å™¨â€”è§£ç å™¨å’Œæ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨ã€‚

##  è¯»å–å’Œé¢„å¤„ç†æ•°æ®

æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›ç‰¹æ®Šç¬¦å·ã€‚å…¶ä¸­â€œ&lt;pad&gt;â€ï¼ˆpaddingï¼‰ç¬¦å·ç”¨æ¥æ·»åŠ åœ¨è¾ƒçŸ­åºåˆ—åï¼Œç›´åˆ°æ¯ä¸ªåºåˆ—ç­‰é•¿ï¼Œè€Œâ€œ&lt;bos&gt;â€å’Œâ€œ&lt;eos&gt;â€ç¬¦å·åˆ†åˆ«è¡¨ç¤ºåºåˆ—çš„å¼€å§‹å’Œç»“æŸã€‚



```python
!tar -xf d2lzh_pytorch.tar
```


```python
import collections
import os
import io
import math
import torch
from torch import nn
import torch.nn.functional as F
import torchtext.vocab as Vocab
import torch.utils.data as Data

import sys
# sys.path.append("..") 
import d2lzh_pytorch as d2l

PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(torch.__version__, device)
```

    1.5.0 cpu


æ¥ç€å®šä¹‰ä¸¤ä¸ªè¾…åŠ©å‡½æ•°å¯¹åé¢è¯»å–çš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚


```python
# å°†ä¸€ä¸ªåºåˆ—ä¸­æ‰€æœ‰çš„è¯è®°å½•åœ¨all_tokensä¸­ä»¥ä¾¿ä¹‹åæ„é€ è¯å…¸ï¼Œç„¶ååœ¨è¯¥åºåˆ—åé¢æ·»åŠ PADç›´åˆ°åºåˆ—
# é•¿åº¦å˜ä¸ºmax_seq_lenï¼Œç„¶åå°†åºåˆ—ä¿å­˜åœ¨all_seqsä¸­
def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):
    all_tokens.extend(seq_tokens)
    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)
    all_seqs.append(seq_tokens)

# ä½¿ç”¨æ‰€æœ‰çš„è¯æ¥æ„é€ è¯å…¸ã€‚å¹¶å°†æ‰€æœ‰åºåˆ—ä¸­çš„è¯å˜æ¢ä¸ºè¯ç´¢å¼•åæ„é€ Tensor
def build_data(all_tokens, all_seqs):
    vocab = Vocab.Vocab(collections.Counter(all_tokens),
                        specials=[PAD, BOS, EOS])
    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]
    return vocab, torch.tensor(indices)
```

ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªå¾ˆå°çš„æ³•è¯­â€”è‹±è¯­æ•°æ®é›†ã€‚åœ¨è¿™ä¸ªæ•°æ®é›†é‡Œï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€å¯¹æ³•è¯­å¥å­å’Œå®ƒå¯¹åº”çš„è‹±è¯­å¥å­ï¼Œä¸­é—´ä½¿ç”¨`'\t'`éš”å¼€ã€‚åœ¨è¯»å–æ•°æ®æ—¶ï¼Œæˆ‘ä»¬åœ¨å¥æœ«é™„ä¸Šâ€œ&lt;eos&gt;â€ç¬¦å·ï¼Œå¹¶å¯èƒ½é€šè¿‡æ·»åŠ â€œ&lt;pad&gt;â€ç¬¦å·ä½¿æ¯ä¸ªåºåˆ—çš„é•¿åº¦å‡ä¸º`max_seq_len`ã€‚æˆ‘ä»¬ä¸ºæ³•è¯­è¯å’Œè‹±è¯­è¯åˆ†åˆ«åˆ›å»ºè¯å…¸ã€‚æ³•è¯­è¯çš„ç´¢å¼•å’Œè‹±è¯­è¯çš„ç´¢å¼•ç›¸äº’ç‹¬ç«‹ã€‚



```python
def read_data(max_seq_len):
    # inå’Œoutåˆ†åˆ«æ˜¯inputå’Œoutputçš„ç¼©å†™
    in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []
    with io.open('fr-en-small.txt') as f:
        lines = f.readlines()
    for line in lines:
        in_seq, out_seq = line.rstrip().split('\t')
        in_seq_tokens, out_seq_tokens = in_seq.split(' '), out_seq.split(' ')
        if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:
            continue  # å¦‚æœåŠ ä¸ŠEOSåé•¿äºmax_seq_lenï¼Œåˆ™å¿½ç•¥æ‰æ­¤æ ·æœ¬
        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)
        process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)
    in_vocab, in_data = build_data(in_tokens, in_seqs)
    out_vocab, out_data = build_data(out_tokens, out_seqs)
    return in_vocab, out_vocab, Data.TensorDataset(in_data, out_data)
```

å°†åºåˆ—çš„æœ€å¤§é•¿åº¦è®¾æˆ7ï¼Œç„¶åæŸ¥çœ‹è¯»å–åˆ°çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ã€‚è¯¥æ ·æœ¬åˆ†åˆ«åŒ…å«æ³•è¯­è¯ç´¢å¼•åºåˆ—å’Œè‹±è¯­è¯ç´¢å¼•åºåˆ—ã€‚


```python
max_seq_len = 7
in_vocab, out_vocab, dataset = read_data(max_seq_len)
dataset[0]
```




    (tensor([ 5,  4, 45,  3,  2,  0,  0]), tensor([ 8,  4, 27,  3,  2,  0,  0]))



##  å«æ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨â€”è§£ç å™¨

æˆ‘ä»¬å°†ä½¿ç”¨å«æ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨â€”è§£ç å™¨æ¥å°†ä¸€æ®µç®€çŸ­çš„æ³•è¯­ç¿»è¯‘æˆè‹±è¯­ã€‚ä¸‹é¢æˆ‘ä»¬æ¥ä»‹ç»æ¨¡å‹çš„å®ç°ã€‚

###  ç¼–ç å™¨

åœ¨ç¼–ç å™¨ä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥è¯­è¨€çš„è¯ç´¢å¼•é€šè¿‡è¯åµŒå…¥å±‚å¾—åˆ°è¯çš„è¡¨å¾ï¼Œç„¶åè¾“å…¥åˆ°ä¸€ä¸ªå¤šå±‚é—¨æ§å¾ªç¯å•å…ƒä¸­ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨6.5èŠ‚ï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œçš„ç®€æ´å®ç°ï¼‰ä¸­æåˆ°çš„ï¼ŒPyTorchçš„`nn.GRU`å®ä¾‹åœ¨å‰å‘è®¡ç®—åä¹Ÿä¼šåˆ†åˆ«è¿”å›è¾“å‡ºå’Œæœ€ç»ˆæ—¶é—´æ­¥çš„å¤šå±‚éšè—çŠ¶æ€ã€‚å…¶ä¸­çš„è¾“å‡ºæŒ‡çš„æ˜¯æœ€åä¸€å±‚çš„éšè—å±‚åœ¨å„ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå¹¶ä¸æ¶‰åŠè¾“å‡ºå±‚è®¡ç®—ã€‚æ³¨æ„åŠ›æœºåˆ¶å°†è¿™äº›è¾“å‡ºä½œä¸ºé”®é¡¹å’Œå€¼é¡¹ã€‚


```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 drop_prob=0, **kwargs):
        super(Encoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=drop_prob)

    def forward(self, inputs, state):
        # è¾“å…¥å½¢çŠ¶æ˜¯(æ‰¹é‡å¤§å°, æ—¶é—´æ­¥æ•°)ã€‚å°†è¾“å‡ºäº’æ¢æ ·æœ¬ç»´å’Œæ—¶é—´æ­¥ç»´
        embedding = self.embedding(inputs.long()).permute(1, 0, 2) # (seq_len, batch, input_size)
        return self.rnn(embedding, state)

    def begin_state(self):
        return None
```

ä¸‹é¢æˆ‘ä»¬æ¥åˆ›å»ºä¸€ä¸ªæ‰¹é‡å¤§å°ä¸º4ã€æ—¶é—´æ­¥æ•°ä¸º7çš„å°æ‰¹é‡åºåˆ—è¾“å…¥ã€‚è®¾é—¨æ§å¾ªç¯å•å…ƒçš„éšè—å±‚ä¸ªæ•°ä¸º2ï¼Œéšè—å•å…ƒä¸ªæ•°ä¸º16ã€‚ç¼–ç å™¨å¯¹è¯¥è¾“å…¥æ‰§è¡Œå‰å‘è®¡ç®—åè¿”å›çš„è¾“å‡ºå½¢çŠ¶ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)ã€‚é—¨æ§å¾ªç¯å•å…ƒåœ¨æœ€ç»ˆæ—¶é—´æ­¥çš„å¤šå±‚éšè—çŠ¶æ€çš„å½¢çŠ¶ä¸º(éšè—å±‚ä¸ªæ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)ã€‚å¯¹äºé—¨æ§å¾ªç¯å•å…ƒæ¥è¯´ï¼Œ`state`å°±æ˜¯ä¸€ä¸ªå…ƒç´ ï¼Œå³éšè—çŠ¶æ€ï¼›å¦‚æœä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ï¼Œ`state`æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸¤ä¸ªå…ƒç´ å³éšè—çŠ¶æ€å’Œè®°å¿†ç»†èƒã€‚


```python
encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)
output, state = encoder(torch.zeros((4, 7)), encoder.begin_state())
output.shape, state.shape # GRUçš„stateæ˜¯h, è€ŒLSTMçš„æ˜¯ä¸€ä¸ªå…ƒç»„(h, c)
```




    (torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))



###  æ³¨æ„åŠ›æœºåˆ¶

æˆ‘ä»¬å°†å®ç°10.11èŠ‚ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ä¸­å®šä¹‰çš„å‡½æ•°$a$ï¼šå°†è¾“å…¥è¿ç»“åé€šè¿‡å«å•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºå˜æ¢ã€‚å…¶ä¸­éšè—å±‚çš„è¾“å…¥æ˜¯è§£ç å™¨çš„éšè—çŠ¶æ€ä¸ç¼–ç å™¨åœ¨æ‰€æœ‰æ—¶é—´æ­¥ä¸Šéšè—çŠ¶æ€çš„ä¸€ä¸€è¿ç»“ï¼Œä¸”ä½¿ç”¨tanhå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚è¾“å‡ºå±‚çš„è¾“å‡ºä¸ªæ•°ä¸º1ã€‚ä¸¤ä¸ª`Linear`å®ä¾‹å‡ä¸ä½¿ç”¨åå·®ã€‚å…¶ä¸­å‡½æ•°$a$å®šä¹‰é‡Œå‘é‡$\boldsymbol{v}$çš„é•¿åº¦æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå³`attention_size`ã€‚


```python
def attention_model(input_size, attention_size):
    model = nn.Sequential(nn.Linear(input_size, attention_size, bias=False),
                          nn.Tanh(),
                          nn.Linear(attention_size, 1, bias=False))
    return model
```

æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥åŒ…æ‹¬æŸ¥è¯¢é¡¹ã€é”®é¡¹å’Œå€¼é¡¹ã€‚è®¾ç¼–ç å™¨å’Œè§£ç å™¨çš„éšè—å•å…ƒä¸ªæ•°ç›¸åŒã€‚è¿™é‡Œçš„æŸ¥è¯¢é¡¹ä¸ºè§£ç å™¨åœ¨ä¸Šä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)ï¼›é”®é¡¹å’Œå€¼é¡¹å‡ä¸ºç¼–ç å™¨åœ¨æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)ã€‚æ³¨æ„åŠ›æœºåˆ¶è¿”å›å½“å‰æ—¶é—´æ­¥çš„èƒŒæ™¯å˜é‡ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)ã€‚


```python
def attention_forward(model, enc_states, dec_state):
    """
    enc_states: (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)
    dec_state: (æ‰¹é‡å¤§å°, éšè—å•å…ƒä¸ªæ•°)
    """
    # å°†è§£ç å™¨éšè—çŠ¶æ€å¹¿æ’­åˆ°å’Œç¼–ç å™¨éšè—çŠ¶æ€å½¢çŠ¶ç›¸åŒåè¿›è¡Œè¿ç»“
    dec_states = dec_state.unsqueeze(dim=0).expand_as(enc_states)
    enc_and_dec_states = torch.cat((enc_states, dec_states), dim=2)
    e = model(enc_and_dec_states)  # å½¢çŠ¶ä¸º(æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, 1)
    alpha = F.softmax(e, dim=0)  # åœ¨æ—¶é—´æ­¥ç»´åº¦åšsoftmaxè¿ç®—
    return (alpha * enc_states).sum(dim=0)  # è¿”å›èƒŒæ™¯å˜é‡
```

åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œç¼–ç å™¨çš„æ—¶é—´æ­¥æ•°ä¸º10ï¼Œæ‰¹é‡å¤§å°ä¸º4ï¼Œç¼–ç å™¨å’Œè§£ç å™¨çš„éšè—å•å…ƒä¸ªæ•°å‡ä¸º8ã€‚æ³¨æ„åŠ›æœºåˆ¶è¿”å›ä¸€ä¸ªå°æ‰¹é‡çš„èƒŒæ™¯å‘é‡ï¼Œæ¯ä¸ªèƒŒæ™¯å‘é‡çš„é•¿åº¦ç­‰äºç¼–ç å™¨çš„éšè—å•å…ƒä¸ªæ•°ã€‚å› æ­¤è¾“å‡ºçš„å½¢çŠ¶ä¸º(4, 8)ã€‚


```python
seq_len, batch_size, num_hiddens = 10, 4, 8
model = attention_model(2*num_hiddens, 10) 
enc_states = torch.zeros((seq_len, batch_size, num_hiddens))
dec_state = torch.zeros((batch_size, num_hiddens))
attention_forward(model, enc_states, dec_state).shape
```




    torch.Size([4, 8])



###  å«æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨

æˆ‘ä»¬ç›´æ¥å°†ç¼–ç å™¨åœ¨æœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºè§£ç å™¨çš„åˆå§‹éšè—çŠ¶æ€ã€‚è¿™è¦æ±‚ç¼–ç å™¨å’Œè§£ç å™¨çš„å¾ªç¯ç¥ç»ç½‘ç»œä½¿ç”¨ç›¸åŒçš„éšè—å±‚ä¸ªæ•°å’Œéšè—å•å…ƒä¸ªæ•°ã€‚

åœ¨è§£ç å™¨çš„å‰å‘è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡åˆšåˆšä»‹ç»çš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å¾—åˆ°å½“å‰æ—¶é—´æ­¥çš„èƒŒæ™¯å‘é‡ã€‚ç”±äºè§£ç å™¨çš„è¾“å…¥æ¥è‡ªè¾“å‡ºè¯­è¨€çš„è¯ç´¢å¼•ï¼Œæˆ‘ä»¬å°†è¾“å…¥é€šè¿‡è¯åµŒå…¥å±‚å¾—åˆ°è¡¨å¾ï¼Œç„¶åå’ŒèƒŒæ™¯å‘é‡åœ¨ç‰¹å¾ç»´è¿ç»“ã€‚æˆ‘ä»¬å°†è¿ç»“åçš„ç»“æœä¸ä¸Šä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€é€šè¿‡é—¨æ§å¾ªç¯å•å…ƒè®¡ç®—å‡ºå½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºä¸éšè—çŠ¶æ€ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¾“å‡ºé€šè¿‡å…¨è¿æ¥å±‚å˜æ¢ä¸ºæœ‰å…³å„ä¸ªè¾“å‡ºè¯çš„é¢„æµ‹ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, è¾“å‡ºè¯å…¸å¤§å°)ã€‚


```python
class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 attention_size, drop_prob=0):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.attention = attention_model(2*num_hiddens, attention_size)
        # GRUçš„è¾“å…¥åŒ…å«attentionè¾“å‡ºçš„cå’Œå®é™…è¾“å…¥, æ‰€ä»¥å°ºå¯¸æ˜¯ num_hiddens+embed_size
        self.rnn = nn.GRU(num_hiddens + embed_size, num_hiddens, 
                          num_layers, dropout=drop_prob)
        self.out = nn.Linear(num_hiddens, vocab_size)

    def forward(self, cur_input, state, enc_states):
        """
        cur_input shape: (batch, )
        state shape: (num_layers, batch, num_hiddens)
        """
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—èƒŒæ™¯å‘é‡
        c = attention_forward(self.attention, enc_states, state[-1])
        # å°†åµŒå…¥åçš„è¾“å…¥å’ŒèƒŒæ™¯å‘é‡åœ¨ç‰¹å¾ç»´è¿ç»“, (æ‰¹é‡å¤§å°, num_hiddens+embed_size)
        input_and_c = torch.cat((self.embedding(cur_input), c), dim=1) 
        # ä¸ºè¾“å…¥å’ŒèƒŒæ™¯å‘é‡çš„è¿ç»“å¢åŠ æ—¶é—´æ­¥ç»´ï¼Œæ—¶é—´æ­¥ä¸ªæ•°ä¸º1
        output, state = self.rnn(input_and_c.unsqueeze(0), state)
        # ç§»é™¤æ—¶é—´æ­¥ç»´ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, è¾“å‡ºè¯å…¸å¤§å°)
        output = self.out(output).squeeze(dim=0)
        return output, state

    def begin_state(self, enc_state):
        # ç›´æ¥å°†ç¼–ç å™¨æœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºè§£ç å™¨çš„åˆå§‹éšè—çŠ¶æ€
        return enc_state
```

##  è®­ç»ƒæ¨¡å‹

æˆ‘ä»¬å…ˆå®ç°`batch_loss`å‡½æ•°è®¡ç®—ä¸€ä¸ªå°æ‰¹é‡çš„æŸå¤±ã€‚è§£ç å™¨åœ¨æœ€åˆæ—¶é—´æ­¥çš„è¾“å…¥æ˜¯ç‰¹æ®Šå­—ç¬¦`BOS`ã€‚ä¹‹åï¼Œè§£ç å™¨åœ¨æŸæ—¶é—´æ­¥çš„è¾“å…¥ä¸ºæ ·æœ¬è¾“å‡ºåºåˆ—åœ¨ä¸Šä¸€æ—¶é—´æ­¥çš„è¯ï¼Œå³å¼ºåˆ¶æ•™å­¦ã€‚æ­¤å¤–ï¼ŒåŒ10.3èŠ‚ï¼ˆword2vecçš„å®ç°ï¼‰ä¸­çš„å®ç°ä¸€æ ·ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä¹Ÿä½¿ç”¨æ©ç å˜é‡é¿å…å¡«å……é¡¹å¯¹æŸå¤±å‡½æ•°è®¡ç®—çš„å½±å“ã€‚


```python
def batch_loss(encoder, decoder, X, Y, loss):
    batch_size = X.shape[0]
    enc_state = encoder.begin_state()
    enc_outputs, enc_state = encoder(X, enc_state)
    # åˆå§‹åŒ–è§£ç å™¨çš„éšè—çŠ¶æ€
    dec_state = decoder.begin_state(enc_state)
    # è§£ç å™¨åœ¨æœ€åˆæ—¶é—´æ­¥çš„è¾“å…¥æ˜¯BOS
    dec_input = torch.tensor([out_vocab.stoi[BOS]] * batch_size)
    # æˆ‘ä»¬å°†ä½¿ç”¨æ©ç å˜é‡maskæ¥å¿½ç•¥æ‰æ ‡ç­¾ä¸ºå¡«å……é¡¹PADçš„æŸå¤±, åˆå§‹å…¨1
    mask, num_not_pad_tokens = torch.ones(batch_size,), 0
    l = torch.tensor([0.0])
    for y in Y.permute(1,0): # Y shape: (batch, seq_len)
        dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)
        l = l + (mask * loss(dec_output, y)).sum()
        dec_input = y  # ä½¿ç”¨å¼ºåˆ¶æ•™å­¦
        num_not_pad_tokens += mask.sum().item()
        # EOSåé¢å…¨æ˜¯PAD. ä¸‹é¢ä¸€è¡Œä¿è¯ä¸€æ—¦é‡åˆ°EOSæ¥ä¸‹æ¥çš„å¾ªç¯ä¸­maskå°±ä¸€ç›´æ˜¯0
        mask = mask * (y != out_vocab.stoi[EOS]).float()
    return l / num_not_pad_tokens
```

åœ¨è®­ç»ƒå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶è¿­ä»£ç¼–ç å™¨å’Œè§£ç å™¨çš„æ¨¡å‹å‚æ•°ã€‚


```python
def train(encoder, decoder, dataset, lr, batch_size, num_epochs):
    enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)
    dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)

    loss = nn.CrossEntropyLoss(reduction='none')
    data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)
    for epoch in range(num_epochs):
        l_sum = 0.0
        for X, Y in data_iter:
            enc_optimizer.zero_grad()
            dec_optimizer.zero_grad()
            l = batch_loss(encoder, decoder, X, Y, loss)
            l.backward()
            enc_optimizer.step()
            dec_optimizer.step()
            l_sum += l.item()
        if (epoch + 1) % 10 == 0:
            print("epoch %d, loss %.3f" % (epoch + 1, l_sum / len(data_iter)))
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶è®¾ç½®è¶…å‚æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å°±å¯ä»¥è®­ç»ƒæ¨¡å‹äº†ã€‚


```python
embed_size, num_hiddens, num_layers = 64, 64, 2
attention_size, drop_prob, lr, batch_size, num_epochs = 10, 0.5, 0.01, 2, 50
encoder = Encoder(len(in_vocab), embed_size, num_hiddens, num_layers,
                  drop_prob)
decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers,
                  attention_size, drop_prob)
train(encoder, decoder, dataset, lr, batch_size, num_epochs)
```

    epoch 10, loss 0.474
    epoch 20, loss 0.203
    epoch 30, loss 0.086
    epoch 40, loss 0.051
    epoch 50, loss 0.028


##  é¢„æµ‹ä¸å®šé•¿çš„åºåˆ—

åœ¨10.10èŠ‚ï¼ˆæŸæœç´¢ï¼‰ä¸­æˆ‘ä»¬ä»‹ç»äº†3ç§æ–¹æ³•æ¥ç”Ÿæˆè§£ç å™¨åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºã€‚è¿™é‡Œæˆ‘ä»¬å®ç°æœ€ç®€å•çš„è´ªå©ªæœç´¢ã€‚


```python
def translate(encoder, decoder, input_seq, max_seq_len):
    """
    ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹è¿›è¡Œåºåˆ—ç¿»è¯‘ã€‚

    Args:
    - encoder: ç¼–ç å™¨æ¨¡å‹å¯¹è±¡ã€‚
    - decoder: è§£ç å™¨æ¨¡å‹å¯¹è±¡ã€‚
    - input_seq (str): è¦ç¿»è¯‘çš„è¾“å…¥åºåˆ—ã€‚
    - max_seq_len (int): è¾“å‡ºåºåˆ—çš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚

    Returns:
    - output_tokens (list): ç¿»è¯‘åçš„è¾“å‡ºåºåˆ—çš„ token åˆ—è¡¨ã€‚
    """
    
    # åˆ†è¯è¾“å…¥åºåˆ—å¹¶æ·»åŠ  EOS å’Œå¡«å……
    in_tokens = input_seq.split(' ')
    in_tokens += [EOS] + [PAD] * (max_seq_len - len(in_tokens) - 1)
    
    # å°†è¾“å…¥ tokens è½¬æ¢ä¸ºå¼ é‡
    enc_input = torch.tensor([[in_vocab.stoi[tk] for tk in in_tokens]])  # batch=1
    
    # åˆå§‹åŒ–ç¼–ç å™¨çŠ¶æ€
    enc_state = encoder.begin_state()
    
    # ç¼–ç å™¨å‰å‘ä¼ æ’­
    enc_output, enc_state = encoder(enc_input, enc_state)
    
    # ä½¿ç”¨ BOS token åˆå§‹åŒ–è§£ç å™¨è¾“å…¥
    dec_input = torch.tensor([out_vocab.stoi[BOS]])
    
    # ä½¿ç”¨ç¼–ç å™¨çŠ¶æ€åˆå§‹åŒ–è§£ç å™¨çŠ¶æ€
    dec_state = decoder.begin_state(enc_state)
    
    # åˆå§‹åŒ–åˆ—è¡¨ä»¥å­˜å‚¨è¾“å‡º tokens
    output_tokens = []
    
    # è§£ç æœ€å¤š max_seq_len ä¸ª token
    for _ in range(max_seq_len):
        # è§£ç å™¨å‰å‘ä¼ æ’­
        dec_output, dec_state = decoder(dec_input, dec_state, enc_output)
        
        # é¢„æµ‹å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ token
        pred = dec_output.argmax(dim=1)
        
        # å°†é¢„æµ‹çš„ token ç´¢å¼•è½¬æ¢ä¸º token å­—ç¬¦ä¸²
        pred_token = out_vocab.itos[int(pred.item())]
        
        # æ£€æŸ¥æ˜¯å¦ä¸º EOS token
        if pred_token == EOS:
            break  # å¦‚æœé¢„æµ‹åˆ° EOSï¼Œåˆ™åœæ­¢è§£ç 
        
        # å°†é¢„æµ‹çš„ token æ·»åŠ åˆ°è¾“å‡ºåºåˆ—
        output_tokens.append(pred_token)
        
        # æ›´æ–°è§£ç å™¨è¾“å…¥ä»¥å‡†å¤‡ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
        dec_input = pred
    
    return output_tokens

```

ç®€å•æµ‹è¯•ä¸€ä¸‹æ¨¡å‹ã€‚è¾“å…¥æ³•è¯­å¥å­â€œils regardent.â€ï¼Œç¿»è¯‘åçš„è‹±è¯­å¥å­åº”è¯¥æ˜¯â€œthey are watching.â€ã€‚


```python
input_seq = 'ils regardent .'
translate(encoder, decoder, input_seq, max_seq_len)
```




    ['they', 'are', 'watching', '.']



##  è¯„ä»·ç¿»è¯‘ç»“æœ

è¯„ä»·æœºå™¨ç¿»è¯‘ç»“æœé€šå¸¸ä½¿ç”¨BLEUï¼ˆBilingual Evaluation Understudyï¼‰[1]ã€‚å¯¹äºæ¨¡å‹é¢„æµ‹åºåˆ—ä¸­ä»»æ„çš„å­åºåˆ—ï¼ŒBLEUè€ƒå¯Ÿè¿™ä¸ªå­åºåˆ—æ˜¯å¦å‡ºç°åœ¨æ ‡ç­¾åºåˆ—ä¸­ã€‚

å…·ä½“æ¥è¯´ï¼Œè®¾è¯æ•°ä¸º$n$çš„å­åºåˆ—çš„ç²¾åº¦ä¸º$p_n$ã€‚å®ƒæ˜¯é¢„æµ‹åºåˆ—ä¸æ ‡ç­¾åºåˆ—åŒ¹é…è¯æ•°ä¸º$n$çš„å­åºåˆ—çš„æ•°é‡ä¸é¢„æµ‹åºåˆ—ä¸­è¯æ•°ä¸º$n$çš„å­åºåˆ—çš„æ•°é‡ä¹‹æ¯”ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾æ ‡ç­¾åºåˆ—ä¸º$A$ã€$B$ã€$C$ã€$D$ã€$E$ã€$F$ï¼Œé¢„æµ‹åºåˆ—ä¸º$A$ã€$B$ã€$B$ã€$C$ã€$D$ï¼Œé‚£ä¹ˆ$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$ã€‚è®¾$len_{\text{label}}$å’Œ$len_{\text{pred}}$åˆ†åˆ«ä¸ºæ ‡ç­¾åºåˆ—å’Œé¢„æµ‹åºåˆ—çš„è¯æ•°ï¼Œé‚£ä¹ˆï¼ŒBLEUçš„å®šä¹‰ä¸º

$$ \exp\left(\min\left(0, 1 - \frac{len_{\text{label}}}{len_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},$$

å…¶ä¸­$k$æ˜¯æˆ‘ä»¬å¸Œæœ›åŒ¹é…çš„å­åºåˆ—çš„æœ€å¤§è¯æ•°ã€‚å¯ä»¥çœ‹åˆ°å½“é¢„æµ‹åºåˆ—å’Œæ ‡ç­¾åºåˆ—å®Œå…¨ä¸€è‡´æ—¶ï¼ŒBLEUä¸º1ã€‚

å› ä¸ºåŒ¹é…è¾ƒé•¿å­åºåˆ—æ¯”åŒ¹é…è¾ƒçŸ­å­åºåˆ—æ›´éš¾ï¼ŒBLEUå¯¹åŒ¹é…è¾ƒé•¿å­åºåˆ—çš„ç²¾åº¦èµ‹äºˆäº†æ›´å¤§æƒé‡ã€‚ä¾‹å¦‚ï¼Œå½“$p_n$å›ºå®šåœ¨0.5æ—¶ï¼Œéšç€$n$çš„å¢å¤§ï¼Œ$0.5^{1/2} \approx 0.7, 0.5^{1/4} \approx 0.84, 0.5^{1/8} \approx 0.92, 0.5^{1/16} \approx 0.96$ã€‚å¦å¤–ï¼Œæ¨¡å‹é¢„æµ‹è¾ƒçŸ­åºåˆ—å¾€å¾€ä¼šå¾—åˆ°è¾ƒé«˜$p_n$å€¼ã€‚å› æ­¤ï¼Œä¸Šå¼ä¸­è¿ä¹˜é¡¹å‰é¢çš„ç³»æ•°æ˜¯ä¸ºäº†æƒ©ç½šè¾ƒçŸ­çš„è¾“å‡ºè€Œè®¾çš„ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå½“$k=2$æ—¶ï¼Œå‡è®¾æ ‡ç­¾åºåˆ—ä¸º$A$ã€$B$ã€$C$ã€$D$ã€$E$ã€$F$ï¼Œè€Œé¢„æµ‹åºåˆ—ä¸º$A$ã€$B$ã€‚è™½ç„¶$p_1 = p_2 = 1$ï¼Œä½†æƒ©ç½šç³»æ•°$\exp(1-6/2) \approx 0.14$ï¼Œå› æ­¤BLEUä¹Ÿæ¥è¿‘0.14ã€‚

ä¸‹é¢æ¥å®ç°BLEUçš„è®¡ç®—ã€‚



```python
import math
import collections

def bleu(pred_tokens, label_tokens, k):
    """
    è®¡ç®— BLEU åˆ†æ•°ä»¥è¯„ä¼°é¢„æµ‹åºåˆ—ä¸å‚è€ƒåºåˆ—ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

    Args:
    - pred_tokens (list): é¢„æµ‹çš„ token åˆ—è¡¨ã€‚
    - label_tokens (list): å‚è€ƒçš„ token åˆ—è¡¨ã€‚
    - k (int): æœ€å¤§çš„ n-gram è®¡ç®—é˜¶æ•°ã€‚

    Returns:
    - score (float): è®¡ç®—å¾—åˆ°çš„ BLEU åˆ†æ•°ã€‚
    """
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    
    # é•¿åº¦æƒ©ç½šé¡¹
    score = math.exp(min(0, 1 - len_label / len_pred))
    
    # è®¡ç®—æ¯ä¸ª n-gram é˜¶æ•°ä» 1 åˆ° k çš„ç²¾åº¦
    for n in range(1, k + 1):
        num_matches, label_subs = 0, collections.defaultdict(int)
        
        # ç»Ÿè®¡å‚è€ƒåºåˆ—ä¸­çš„ n-grams
        for i in range(len_label - n + 1):
            label_subs[''.join(label_tokens[i: i + n])] += 1
        
        # ç»Ÿè®¡é¢„æµ‹åºåˆ—ä¸­ä¸å‚è€ƒåºåˆ—åŒ¹é…çš„ n-grams
        for i in range(len_pred - n + 1):
            if label_subs[''.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[''.join(pred_tokens[i: i + n])] -= 1
        
        # è®¡ç®—å½“å‰ n-gram é˜¶æ•°çš„ç²¾åº¦
        precision = num_matches / (len_pred - n + 1)
        
        # åº”ç”¨é•¿åº¦æƒ©ç½šå› å­
        score *= math.pow(precision, math.pow(0.5, n))
    
    return score
```

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªè¾…åŠ©æ‰“å°å‡½æ•°ã€‚


```python
def score(input_seq, label_seq, k):
    pred_tokens = translate(encoder, decoder, input_seq, max_seq_len)
    label_tokens = label_seq.split(' ')
    print('bleu %.3f, predict: %s' % (bleu(pred_tokens, label_tokens, k),
                                      ' '.join(pred_tokens)))
```

é¢„æµ‹æ­£ç¡®åˆ™åˆ†æ•°ä¸º1ã€‚


```python
score('ils regardent .', 'they are watching .', k=2)
```

    bleu 1.000, predict: they are watching .



```python
score('ils sont canadienne .', 'they are canadian .', k=2)
```

    bleu 0.658, predict: they are russian .


## å°ç»“

* å¯ä»¥å°†ç¼–ç å™¨â€”è§£ç å™¨å’Œæ³¨æ„åŠ›æœºåˆ¶åº”ç”¨äºæœºå™¨ç¿»è¯‘ä¸­ã€‚
* BLEUå¯ä»¥ç”¨æ¥è¯„ä»·ç¿»è¯‘ç»“æœã€‚


## å‚è€ƒæ–‡çŒ®

[1] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311-318). Association for Computational Linguistics.

[2] WMT. http://www.statmt.org/wmt14/translation-task.html

[3] Tatoeba Project. http://www.manythings.org/anki/

# Japanese-Chinese Machine Translation Model with Transformer & PyTorch

A tutorial using Jupyter Notebook, PyTorch, Torchtext, and SentencePiece

## Import required packages
Firstly, letâ€™s make sure we have the below packages installed in our system, if you found that some packages are missing, make sure to install them.


```python
# å®‰è£… torchtext åº“
!pip install torchtext

# å¯¼å…¥ torchtext åº“
import torchtext

# æ‰“å° torchtext ç‰ˆæœ¬
print(torchtext.__version__)

# å®‰è£… torchtext åº“
!pip install torchtext

# å®‰è£… sentencepiece åº“
!pip install sentencepiece

# å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„ torchtext åº“
!pip install torchtext==0.6.0

# å®‰è£… pandas åº“
!pip install pandas
```

    Looking in indexes: http://mirrors.aliyun.com/pypi/simple
    Requirement already satisfied: pandas in ./miniconda3/lib/python3.8/site-packages (2.0.3)
    Requirement already satisfied: numpy>=1.20.3 in ./miniconda3/lib/python3.8/site-packages (from pandas) (1.24.2)
    Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2.8.2)
    Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2022.7.1)
    Requirement already satisfied: tzdata>=2022.1 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2024.1)
    Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m

å®éªŒç¯å¢ƒ

![image-20240623143608781](/Users/wangyang/Library/Application Support/typora-user-images/image-20240623143608781.png)

```python
import math
import torchtext
import torch
import torch.nn as nn
from torch import Tensor
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
from collections import Counter
from torchtext.vocab import Vocab
from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer
import io
import time
import pandas as pd
import numpy as np
import pickle
import tqdm
import sentencepiece as spm
torch.manual_seed(0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# print(torch.cuda.get_device_name(0)) ## å¦‚æœä½ æœ‰GPUï¼Œè¯·åœ¨ä½ è‡ªå·±çš„ç”µè„‘ä¸Šå°è¯•è¿è¡Œè¿™ä¸€å¥—ä»£ç 


```


```python
device
```


    device(type='cuda')



## Get the parallel dataset
In this tutorial, we will use the Japanese-English parallel dataset downloaded from JParaCrawl![http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl] which is described as the â€œlargest publicly available English-Japanese parallel corpus created by NTT. It was created by largely crawling the web and automatically aligning parallel sentences.â€ You can also see the paper here.


```python
import pandas as pd

# ä»æŒ‡å®šè·¯å¾„è¯»å– TSV æ–‡ä»¶ï¼Œå¹¶å°†å…¶åŠ è½½åˆ° DataFrame ä¸­
# ä½¿ç”¨ \\t ä½œä¸ºåˆ†éš”ç¬¦ï¼ŒæŒ‡å®š engine ä¸º 'python' å¹¶ä¸”ä¸ä½¿ç”¨ä»»ä½•åˆ—ä½œä¸º header
df = pd.read_csv('./zh-ja.bicleaner05.txt', sep='\\t', engine='python', header=None)

# æå– DataFrame çš„ç¬¬ä¸‰åˆ—ï¼ˆç´¢å¼•ä¸º 2ï¼‰å¹¶è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œèµ‹å€¼ç»™ trainen
trainen = df[2].values.tolist()  #[:10000]

# æå– DataFrame çš„ç¬¬å››åˆ—ï¼ˆç´¢å¼•ä¸º 3ï¼‰å¹¶è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œèµ‹å€¼ç»™ trainja
trainja = df[3].values.tolist()  #[:10000]

# ç§»é™¤ trainen åˆ—è¡¨ä¸­çš„ç¬¬ 5972 ä¸ªå…ƒç´ 
# trainen.pop(5972)

# ç§»é™¤ trainja åˆ—è¡¨ä¸­çš„ç¬¬ 5972 ä¸ªå…ƒç´ 
# trainja.pop(5972)
```

After importing all the Japanese and their English counterparts, I deleted the last data in the dataset because it has a missing value. In total, the number of sentences in both trainen and trainja is 5,973,071, however, for learning purposes, it is often recommended to sample the data and make sure everything is working as intended, before using all the data at once, to save time.



Here is an example of sentence contained in the dataset.




```python
print(trainen[500])
print(trainja[500])
```

    Chinese HS Code Harmonized Code System < HSç¼–ç  2905 æ— ç¯é†‡åŠå…¶å¤åŒ–ã€ç£ºåŒ–ã€ç¡åŒ–æˆ–äºšç¡åŒ–è¡ç”Ÿç‰© HS Code List (Harmonized System Code) for US, UK, EU, China, India, France, Japan, Russia, Germany, Korea, Canada ...
    Japanese HS Code Harmonized Code System < HSã‚³ãƒ¼ãƒ‰ 2905 éç’°å¼ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«ä¸¦ã³ã«ãã®ãƒãƒ­ã‚²ãƒ³åŒ–èª˜å°ä½“ã€ã‚¹ãƒ«ãƒ›ãƒ³åŒ–èª˜å°ä½“ã€ãƒ‹ãƒˆãƒ­åŒ–èª˜å°ä½“åŠã³ãƒ‹ãƒˆãƒ­ã‚½åŒ–èª˜å°ä½“ HS Code List (Harmonized System Code) for US, UK, EU, China, India, France, Japan, Russia, Germany, Korea, Canada ...


We can also use different parallel datasets to follow along with this article, just make sure that we can process the data into the two lists of strings as shown above, containing the Japanese and English sentences.

## Prepare the tokenizers
Unlike English or other alphabetical languages, a Japanese sentence does not contain whitespaces to separate the words. We can use the tokenizers provided by JParaCrawl which was created using SentencePiece for both Japanese and English, you can visit the JParaCrawl website to download them, or click here.


```python
en_tokenizer = spm.SentencePieceProcessor(model_file='./spm.en.nopretok.model')
ja_tokenizer = spm.SentencePieceProcessor(model_file='./spm.ja.nopretok.model')
```

After the tokenizers are loaded, you can test them, for example, by executing the below code.




```python
en_tokenizer.encode("All residents aged 20 to 59 years who live in Japan must enroll in public pension system.")

```




    [227,
     2980,
     8863,
     373,
     8,
     9381,
     126,
     91,
     649,
     11,
     93,
     240,
     19228,
     11,
     419,
     14926,
     102,
     5]




```python


```


```python
ja_tokenizer.encode("å¹´é‡‘ æ—¥æœ¬ã«ä½ã‚“ã§ã„ã‚‹20æ­³~60æ­³ã®å…¨ã¦ã®äººã¯ã€å…¬çš„å¹´é‡‘åˆ¶åº¦ã«åŠ å…¥ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚")
```




    [4,
     31,
     346,
     912,
     10050,
     222,
     1337,
     372,
     820,
     4559,
     858,
     750,
     3,
     13118,
     31,
     346,
     2000,
     10,
     8978,
     5461,
     5]



## Build the TorchText Vocab objects and convert the sentences into Torch tensors
Using the tokenizers and raw sentences, we then build the Vocab object imported from TorchText. This process can take a few seconds or minutes depending on the size of our dataset and computing power. Different tokenizer can also affect the time needed to build the vocab, I tried several other tokenizers for Japanese but SentencePiece seems to be working well and fast enough for me.


```python
from collections import Counter
from torchtext.vocab import Vocab

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ„å»ºè¯æ±‡è¡¨
def build_vocab(sentences, tokenizer):
    # åˆ›å»ºä¸€ä¸ªè®¡æ•°å™¨å¯¹è±¡
    counter = Counter()
    # éå†æ‰€æœ‰å¥å­
    for sentence in sentences:
        # ä½¿ç”¨åˆ†è¯å™¨å¯¹å¥å­è¿›è¡Œç¼–ç å¹¶æ›´æ–°è®¡æ•°å™¨
        counter.update(tokenizer.encode(sentence))
    # åˆ›å»ºè¯æ±‡è¡¨å¯¹è±¡ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])

# ä½¿ç”¨ç»™å®šçš„æ—¥è¯­åˆ†è¯å™¨æ„å»ºæ—¥è¯­è¯æ±‡è¡¨
ja_vocab = build_vocab(trainja, ja_tokenizer)

# ä½¿ç”¨ç»™å®šçš„è‹±è¯­åˆ†è¯å™¨æ„å»ºè‹±è¯­è¯æ±‡è¡¨
en_vocab = build_vocab(trainen, en_tokenizer)
```

After we have the vocabulary objects, we can then use the vocab and the tokenizer objects to build the tensors for our training data.




```python
import torch

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†æ•°æ®
def data_process(ja, en):
    data = []
    # éå†æ—¥è¯­å’Œè‹±è¯­å¥å­çš„å…ƒç»„
    for (raw_ja, raw_en) in zip(ja, en):
        # ä½¿ç”¨æ—¥è¯­è¯æ±‡è¡¨å°†åˆ†è¯åçš„æ—¥è¯­å¥å­ç¼–ç ä¸ºå¼ é‡
        ja_tensor_ = torch.tensor(
            [ja_vocab[token] for token in ja_tokenizer.encode(raw_ja.rstrip("\n"), out_type=str)],
            dtype=torch.long
        )
        # ä½¿ç”¨è‹±è¯­è¯æ±‡è¡¨å°†åˆ†è¯åçš„è‹±è¯­å¥å­ç¼–ç ä¸ºå¼ é‡
        en_tensor_ = torch.tensor(
            [en_vocab[token] for token in en_tokenizer.encode(raw_en.rstrip("\n"), out_type=str)],
            dtype=torch.long
        )
        # å°†ç¼–ç åçš„å¼ é‡å¯¹æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨ä¸­
        data.append((ja_tensor_, en_tensor_))
    # è¿”å›å¤„ç†åçš„æ•°æ®
    return data

# ä½¿ç”¨å¤„ç†å‡½æ•°å¤„ç†è®­ç»ƒæ•°æ®
train_data = data_process(trainja, trainen)

```

## Create the DataLoader object to be iterated during training
Here, I set the BATCH_SIZE to 16 to prevent â€œcuda out of memoryâ€, but this depends on various things such as your machine memory capacity, size of data, etc., so feel free to change the batch size according to your needs (note: the tutorial from PyTorch sets the batch size as 128 using the Multi30k German-English dataset.)


```python
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

# å®šä¹‰æ‰¹å¤„ç†å¤§å°
BATCH_SIZE = 8

# è·å–ç‰¹æ®Šæ ‡è®°çš„ç´¢å¼•
PAD_IDX = ja_vocab['<pad>']
BOS_IDX = ja_vocab['<bos>']
EOS_IDX = ja_vocab['<eos>']

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç”Ÿæˆæ‰¹æ¬¡æ•°æ®
def generate_batch(data_batch):
    ja_batch, en_batch = [], []
    # éå†æ•°æ®æ‰¹æ¬¡ä¸­çš„æ¯å¯¹å¥å­
    for (ja_item, en_item) in data_batch:
        # åœ¨æ—¥è¯­å¥å­çš„å¼€å¤´å’Œç»“å°¾æ·»åŠ  BOS å’Œ EOS æ ‡è®°ï¼Œå¹¶å°†å…¶åŠ å…¥æ‰¹æ¬¡
        ja_batch.append(torch.cat([torch.tensor([BOS_IDX]), ja_item, torch.tensor([EOS_IDX])], dim=0))
        # åœ¨è‹±è¯­å¥å­çš„å¼€å¤´å’Œç»“å°¾æ·»åŠ  BOS å’Œ EOS æ ‡è®°ï¼Œå¹¶å°†å…¶åŠ å…¥æ‰¹æ¬¡
        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))
    # å¯¹æ‰¹æ¬¡ä¸­çš„å¥å­è¿›è¡Œå¡«å……ï¼Œä½¿å…¶å…·æœ‰ç›¸åŒé•¿åº¦
    ja_batch = pad_sequence(ja_batch, padding_value=PAD_IDX)
    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)
    # è¿”å›å¡«å……åçš„æ‰¹æ¬¡æ•°æ®
    return ja_batch, en_batch

# åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®çš„æ‰¹æ¬¡
train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,
                        shuffle=True, collate_fn=generate_batch)

```


## Sequence-to-sequence Transformer
The next couple of codes and text explanations (written in italic) are taken from the original PyTorch tutorial [https://pytorch.org/tutorials/beginner/translation_transformer.html]. I did not make any change except for the BATCH_SIZE and the word de_vocabwhich is changed to ja_vocab.

Transformer is a Seq2Seq model introduced in â€œAttention is all you needâ€ paper for solving machine translation task. Transformer model consists of an encoder and decoder block each containing fixed number of layers.

Encoder processes the input sequence by propagating it, through a series of Multi-head Attention and Feed forward network layers. The output from the Encoder referred to as memory, is fed to the decoder along with target tensors. Encoder and decoder are trained in an end-to-end fashion using teacher forcing technique.


```python
import torch
from torch import nn, Tensor
from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer

# å®šä¹‰ä¸€ä¸ªç±»ï¼Œç”¨äºåºåˆ—åˆ°åºåˆ—çš„ Transformer æ¨¡å‹
class Seq2SeqTransformer(nn.Module):
    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,
                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,
                 dim_feedforward: int = 512, dropout: float = 0.1):
        super(Seq2SeqTransformer, self).__init__()

        # åˆ›å»ºç¼–ç å™¨å±‚
        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,
                                                dim_feedforward=dim_feedforward)
        # åˆ›å»º Transformer ç¼–ç å™¨
        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)

        # åˆ›å»ºè§£ç å™¨å±‚
        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,
                                                dim_feedforward=dim_feedforward)
        # åˆ›å»º Transformer è§£ç å™¨
        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)

        # å®šä¹‰ç”Ÿæˆå™¨çº¿æ€§å±‚ï¼Œå°†ç¼–ç åçš„å‘é‡è½¬æ¢ä¸ºç›®æ ‡è¯æ±‡è¡¨çš„æ¦‚ç‡åˆ†å¸ƒ
        self.generator = nn.Linear(emb_size, tgt_vocab_size)
        
        # å®šä¹‰æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„è¯åµŒå…¥å±‚
        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        
        # å®šä¹‰ä½ç½®ç¼–ç å±‚
        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)

    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,
                tgt_mask: Tensor, src_padding_mask: Tensor,
                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):
        # å¯¹æºè¯­è¨€å¥å­è¿›è¡Œè¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        # å¯¹ç›®æ ‡è¯­è¨€å¥å­è¿›è¡Œè¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        # é€šè¿‡ç¼–ç å™¨å¯¹æºè¯­è¨€å¥å­è¿›è¡Œç¼–ç 
        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)
        # é€šè¿‡è§£ç å™¨å¯¹ç›®æ ‡è¯­è¨€å¥å­è¿›è¡Œè§£ç 
        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,
                                        tgt_padding_mask, memory_key_padding_mask)
        # ç”Ÿæˆæœ€ç»ˆçš„è¾“å‡º
        return self.generator(outs)

    def encode(self, src: Tensor, src_mask: Tensor):
        # å¯¹æºè¯­è¨€å¥å­è¿›è¡Œç¼–ç 
        return self.transformer_encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)

    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):
        # å¯¹ç›®æ ‡è¯­è¨€å¥å­è¿›è¡Œè§£ç 
        return self.transformer_decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)

```

Text tokens are represented by using token embeddings. Positional encoding is added to the token embedding to introduce a notion of word order.




```python
import math
import torch
from torch import nn, Tensor

# å®šä¹‰ä½ç½®ç¼–ç ç±»
class PositionalEncoding(nn.Module):
    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        # è®¡ç®—ä½ç½®ç¼–ç çš„åˆ†æ¯
        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)
        # ç”Ÿæˆä½ç½®åºåˆ—
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(-2)

        # å®šä¹‰ Dropout å±‚
        self.dropout = nn.Dropout(dropout)
        # å°†ä½ç½®ç¼–ç çŸ©é˜µæ³¨å†Œä¸ºç¼“å†²åŒºï¼Œé˜²æ­¢å…¶è¢«ä¼˜åŒ–å™¨æ›´æ–°
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        # å°†ä½ç½®ç¼–ç æ·»åŠ åˆ°è¾“å…¥çš„ token embedding ä¸Šï¼Œå¹¶åº”ç”¨ Dropout
        return self.dropout(token_embedding +
                            self.pos_embedding[:token_embedding.size(0), :])

# å®šä¹‰ TokenEmbedding ç±»
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size: int):
        super(TokenEmbedding, self).__init__()
        # å®šä¹‰åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        # å¯¹è¾“å…¥ tokens è¿›è¡ŒåµŒå…¥ï¼Œå¹¶æ ¹æ®åµŒå…¥å¤§å°è¿›è¡Œç¼©æ”¾
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

```

We create a subsequent word mask to stop a target word from attending to its subsequent words. We also create masks, for masking source and target padding tokens




```python
import torch

# å®šä¹‰è®¾å¤‡ï¼Œé€šå¸¸æ˜¯ 'cuda' æˆ– 'cpu'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ç”Ÿæˆä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µæ©ç ï¼Œç”¨äºè‡ªå›å½’è§£ç å™¨
def generate_square_subsequent_mask(sz):
    # åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œä¸»å¯¹è§’çº¿åŠå…¶ä¸Šæ–¹çš„å…ƒç´ ä¸º 1ï¼Œå…¶ä½™ä¸º 0
    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)
    # å°† 0 å¡«å……ä¸º -infï¼Œå°† 1 å¡«å……ä¸º 0.0
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

# åˆ›å»ºæºå’Œç›®æ ‡çš„æ©ç ï¼Œç”¨äº Transformer æ¨¡å‹
def create_mask(src, tgt):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]

    # ç”Ÿæˆç›®æ ‡åºåˆ—çš„æ©ç 
    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    # æºåºåˆ—çš„æ©ç å…¨ä¸º 0
    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)

    # ç”Ÿæˆæºå’Œç›®æ ‡åºåˆ—çš„å¡«å……æ©ç 
    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)
    
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

```

Define model parameters and instantiate model. è¿™é‡Œæˆ‘ä»¬æœåŠ¡å™¨å®åœ¨æ˜¯è®¡ç®—èƒ½åŠ›æœ‰é™ï¼ŒæŒ‰ç…§ä»¥ä¸‹é…ç½®å¯ä»¥è®­ç»ƒä½†æ˜¯æ•ˆæœåº”è¯¥æ˜¯ä¸è¡Œçš„ã€‚å¦‚æœæƒ³è¦çœ‹åˆ°è®­ç»ƒçš„æ•ˆæœè¯·ä½¿ç”¨ä½ è‡ªå·±çš„å¸¦GPUçš„ç”µè„‘è¿è¡Œè¿™ä¸€å¥—ä»£ç ã€‚

å½“ä½ ä½¿ç”¨è‡ªå·±çš„GPUçš„æ—¶å€™ï¼ŒNUM_ENCODER_LAYERS å’Œ NUM_DECODER_LAYERS è®¾ç½®ä¸º3æˆ–è€…æ›´é«˜ï¼ŒNHEADè®¾ç½®8ï¼ŒEMB_SIZEè®¾ç½®ä¸º512ã€‚

è®¾ç½®è¶…å‚æ•°å’Œæ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬è¯æ±‡è¡¨å¤§å°ã€åµŒå…¥ç»´åº¦ã€å¤´æ•°ã€å‰é¦ˆç½‘ç»œç»´åº¦ã€æ‰¹å¤„ç†å¤§å°ã€ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°ä»¥åŠè®­ç»ƒè½®æ•°ã€‚

åˆå§‹åŒ– `Seq2SeqTransformer` æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ Xavier å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚

å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰ã€‚

å®šä¹‰æŸå¤±å‡½æ•° `CrossEntropyLoss`ï¼Œå¿½ç•¥å¡«å……æ ‡è®°çš„æŸå¤±ã€‚

å®šä¹‰ Adam ä¼˜åŒ–å™¨ï¼Œå¹¶è®¾ç½®å­¦ä¹ ç‡å’Œå…¶ä»–å‚æ•°ã€‚

å®šä¹‰ `train_epoch` å‡½æ•°ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ª epochã€‚å®ƒä¼šå¯¹è¾“å…¥å’Œç›®æ ‡è¿›è¡Œæ©ç å¤„ç†ï¼Œè®¡ç®—æŸå¤±å¹¶è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚

å®šä¹‰ `evaluate` å‡½æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚å®ƒä¼šè®¡ç®—éªŒè¯é›†çš„å¹³å‡æŸå¤±ã€‚


```python
# å®šä¹‰è¶…å‚æ•°å’Œæ¨¡å‹å‚æ•°
SRC_VOCAB_SIZE = len(ja_vocab)
TGT_VOCAB_SIZE = len(en_vocab)
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
BATCH_SIZE = 16
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3
NUM_EPOCHS = 16

# åˆå§‹åŒ– Seq2Seq Transformer æ¨¡å‹
transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,
                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,
                                 FFN_HID_DIM)

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
for p in transformer.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)

# å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
transformer = transformer.to(device)

# å®šä¹‰æŸå¤±å‡½æ•°ï¼Œå¿½ç•¥å¡«å……æ ‡è®°çš„æŸå¤±
loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# å®šä¹‰ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(
    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9
)

# å®šä¹‰è®­ç»ƒä¸€ä¸ª epoch çš„å‡½æ•°
def train_epoch(model, train_iter, optimizer):
    model.train()
    losses = 0
    for idx, (src, tgt) in enumerate(train_iter):
        src = src.to(device)
        tgt = tgt.to(device)

        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

        logits = model(src, tgt_input, src_mask, tgt_mask,
                       src_padding_mask, tgt_padding_mask, src_padding_mask)

        optimizer.zero_grad()

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()

        optimizer.step()
        losses += loss.item()
    return losses / len(train_iter)

# å®šä¹‰è¯„ä¼°å‡½æ•°
def evaluate(model, val_iter):
    model.eval()
    losses = 0
    for idx, (src, tgt) in enumerate(val_iter):
        src = src.to(device)
        tgt = tgt.to(device)

        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

        logits = model(src, tgt_input, src_mask, tgt_mask,
                       src_padding_mask, tgt_padding_mask, src_padding_mask)

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        losses += loss.item()
    return losses / len(val_iter)

```

## Start training
Finally, after preparing the necessary classes and functions, we are ready to train our model. This goes without saying but the time needed to finish training could vary greatly depending on a lot of things such as computing power, parameters, and size of datasets.

When I trained the model using the complete list of sentences from JParaCrawl which has around 5.9 million sentences for each language, it took around 5 hours per epoch using a single NVIDIA GeForce RTX 3070 GPU.

Here is the code:


```python
for epoch in tqdm.tqdm(range(1, NUM_EPOCHS+1)):
  start_time = time.time()
  train_loss = train_epoch(transformer, train_iter, optimizer)
  end_time = time.time()
  print((f"Epoch: {epoch}, Train loss: {train_loss:.3f}, "
          f"Epoch time = {(end_time - start_time):.3f}s"))

```

      0%|          | 0/16 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.
      warnings.warn(
    /root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
      warnings.warn(
      6%|â–‹         | 1/16 [06:26<1:36:32, 386.20s/it]
    
    Epoch: 1, Train loss: 5.088, Epoch time = 386.195s


     12%|â–ˆâ–        | 2/16 [12:56<1:30:38, 388.47s/it]
    
    Epoch: 2, Train loss: 5.069, Epoch time = 390.053s


     19%|â–ˆâ–‰        | 3/16 [19:26<1:24:19, 389.21s/it]
    
    Epoch: 3, Train loss: 5.068, Epoch time = 390.090s


     25%|â–ˆâ–ˆâ–Œ       | 4/16 [25:54<1:17:46, 388.89s/it]
    
    Epoch: 4, Train loss: 4.468, Epoch time = 388.397s


     31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [32:21<1:11:08, 388.03s/it]
    
    Epoch: 5, Train loss: 4.268, Epoch time = 386.505s


     38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [38:45<1:04:28, 386.82s/it]
    
    Epoch: 6, Train loss: 3.734, Epoch time = 384.463s


     44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [45:13<58:03, 387.08s/it]  
    
    Epoch: 7, Train loss: 4.025, Epoch time = 387.606s


     50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [51:40<51:37, 387.17s/it]
    
    Epoch: 8, Train loss: 3.937, Epoch time = 387.375s


     56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [58:08<45:12, 387.51s/it]
    
    Epoch: 9, Train loss: 3.868, Epoch time = 388.239s


     62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10/16 [1:04:37<38:46, 387.78s/it]
    
    Epoch: 10, Train loss: 3.729, Epoch time = 388.387s


     69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [1:08:44<28:43, 344.61s/it]
    
    Epoch: 11, Train loss: 3.662, Epoch time = 246.728s


     75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [1:12:17<20:19, 304.76s/it]
    
    Epoch: 12, Train loss: 3.436, Epoch time = 213.606s


     81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [1:15:49<13:50, 276.73s/it]
    
    Epoch: 13, Train loss: 3.325, Epoch time = 212.219s


     88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [1:19:22<08:34, 257.50s/it]
    
    Epoch: 14, Train loss: 3.298, Epoch time = 213.063s


     94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [1:22:55<04:03, 243.91s/it]
    
    Epoch: 15, Train loss: 3.276, Epoch time = 212.429s


    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [1:26:29<00:00, 324.33s/it]
    
    Epoch: 16, Train loss: 3.229, Epoch time = 213.948s


â€‹    


## Try translating a Japanese sentence using the trained model
First, we create the functions to translate a new sentence, including steps such as to get the Japanese sentence, tokenize, convert to tensors, inference, and then decode the result back into a sentence, but this time in English.


```python
# å®šä¹‰è´ªå©ªè§£ç å‡½æ•°
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    src = src.to(device)
    src_mask = src_mask.to(device)
    # ç¼–ç æºåºåˆ—
    memory = model.encode(src, src_mask)
    # åˆå§‹åŒ–ç›®æ ‡åºåˆ—ï¼Œèµ·å§‹ç¬¦å·
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)
    for i in range(max_len - 1):
        memory = memory.to(device)
        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(device)
        # è§£ç ç›®æ ‡åºåˆ—
        out = model.decode(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        # é€šè¿‡ç”Ÿæˆå™¨é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡
        prob = model.generator(out[:, -1])
        # é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„å•è¯ä½œä¸ºä¸‹ä¸€ä¸ªå•è¯
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()
        # å°†ä¸‹ä¸€ä¸ªå•è¯æ·»åŠ åˆ°ç›®æ ‡åºåˆ—
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == EOS_IDX:
            break
    return ys

# å®šä¹‰ç¿»è¯‘å‡½æ•°
def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):
    model.eval()
    # å°†æºå¥å­ç¼–ç ä¸º token åºåˆ—å¹¶æ·»åŠ èµ·å§‹å’Œç»“æŸç¬¦å·
    tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer.encode(src, out_type=str)] + [EOS_IDX]
    num_tokens = len(tokens)
    src = torch.LongTensor(tokens).reshape(num_tokens, 1)
    src_mask = torch.zeros(num_tokens, num_tokens).type(torch.bool)
    # ä½¿ç”¨è´ªå©ªè§£ç ç”Ÿæˆç›®æ ‡åºåˆ—
    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()
    # å°†ç›®æ ‡åºåˆ—çš„ token è½¬æ¢ä¸ºå•è¯ï¼Œå¹¶ç§»é™¤èµ·å§‹å’Œç»“æŸç¬¦å·
    return " ".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace("<bos>", "").replace("<eos>", "")

```

Then, we can just call the translate function and pass the required parameters.




```python
translate(transformer, "HSã‚³ãƒ¼ãƒ‰ 8515 ã¯ã‚“ã ä»˜ã‘ç”¨ã€ã‚ã†ä»˜ã‘ç”¨åˆã¯æº¶æ¥ç”¨ã®æ©Ÿå™¨(é›»æ°—å¼(é›»æ°—åŠ ç†±ã‚¬ã‚¹å¼ã‚’å«ã‚€ã€‚)", ja_vocab, en_vocab, ja_tokenizer)

```




    '_H S ç”¨ äº _85 15_ ç„Š ã€ç„Š æ¥ è®¾ å¤‡ ã€ç„Š æ¥ ç”µ æ°” å¼ ï¼ˆ åŒ… æ‹¬ ç”µ æ°” åŠ  çƒ­ ) ã€‚'




```python
trainen.pop(5)
```




    'ç¾å›½ è®¾æ–½: åœè½¦åœº, 24å°æ—¶å‰å°, å¥èº«ä¸­å¿ƒ, æŠ¥çº¸, éœ²å°, ç¦çƒŸå®¢æˆ¿, å¹²æ´—, æ— éšœç¢è®¾æ–½, å…è´¹åœè½¦, ä¸Šç½‘æœåŠ¡, ç”µæ¢¯, å¿«é€ŸåŠç†å…¥ä½/é€€æˆ¿æ‰‹ç»­, ä¿é™©ç®±, æš–æ°”, ä¼ çœŸ/å¤å°, è¡Œæå¯„å­˜, æ— çº¿ç½‘ç»œ, å…è´¹æ— çº¿ç½‘ç»œè¿æ¥, é…’åº—å„å¤„ç¦çƒŸ, ç©ºè°ƒ, é˜³å…‰éœ²å°, è‡ªåŠ¨å”®è´§æœº(é¥®å“), è‡ªåŠ¨å”®è´§æœº(é›¶é£Ÿ), æ¯æ—¥æ¸…æ´æœåŠ¡, å†…éƒ¨åœè½¦åœº, ç§äººåœè½¦åœº, WiFi(è¦†ç›–é…’åº—å„å¤„), åœè½¦åº“, æ— éšœç¢åœè½¦åœº, ç®€çŸ­æè¿°Gateway Hotel Santa Monicaé…’åº—è·ç¦»æµ·æ»©2è‹±é‡Œ(3.2å…¬é‡Œ),æä¾›24å°æ—¶å¥èº«æˆ¿ã€‚æ¯é—´å®¢æˆ¿å‡æä¾›å…è´¹WiFi,å®¢äººå¯ä»¥ä½¿ç”¨é…’åº—çš„å…è´¹åœ°ä¸‹åœè½¦åœºã€‚'




```python
trainja.pop(5)
```




    'ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½ æ–½è¨­ãƒ»è¨­å‚™: é§è»Šå ´, 24æ™‚é–“å¯¾å¿œãƒ•ãƒ­ãƒ³ãƒˆ, ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚»ãƒ³ã‚¿ãƒ¼, æ–°è, ãƒ†ãƒ©ã‚¹, ç¦ç…™ãƒ«ãƒ¼ãƒ , ãƒ‰ãƒ©ã‚¤ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°, ãƒãƒªã‚¢ãƒ•ãƒªãƒ¼, ç„¡æ–™é§è»Šå ´, ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ, ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼, ã‚¨ã‚¯ã‚¹ãƒ—ãƒ¬ã‚¹ãƒ»ãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³ / ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆ, ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒœãƒƒã‚¯ã‚¹, æš–æˆ¿, FAX / ã‚³ãƒ”ãƒ¼, è·ç‰©é ã‹ã‚Š, Wi-Fi, ç„¡æ–™Wi-Fi, å…¨é¤¨ç¦ç…™, ã‚¨ã‚¢ã‚³ãƒ³, ã‚µãƒ³ãƒ†ãƒ©ã‚¹, è‡ªè²©æ©Ÿ(ãƒ‰ãƒªãƒ³ã‚¯é¡), è‡ªè²©æ©Ÿ(ã‚¹ãƒŠãƒƒã‚¯é¡), å®¢å®¤æ¸…æƒã‚µãƒ¼ãƒ“ã‚¹(æ¯æ—¥), æ•·åœ°å†…é§è»Šå ´, å°‚ç”¨é§è»Šå ´, Wi-Fi(é¤¨å†…å…¨åŸŸ), ç«‹ä½“é§è»Šå ´, éšœå®³è€…ç”¨é§è»Šå ´, çŸ­ã„èª¬æ˜Gateway Hotel Santa Monicaã¯ãƒ“ãƒ¼ãƒã‹ã‚‰3.2kmã®å ´æ‰€ã«ä½ç½®ã—ã€24æ™‚é–“åˆ©ç”¨å¯èƒ½ãªã‚¸ãƒ ã€ç„¡æ–™Wi-Fiä»˜ãã®ãŠéƒ¨å±‹ã€ç„¡æ–™ã®åœ°ä¸‹é§è»Šå ´ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚'




```python

```

## Save the Vocab objects and trained model
Finally, after the training has finished, we will save the Vocab objects (en_vocab and ja_vocab) first, using Pickle.


```python
import pickle
# open a file, where you want to store the data
file = open('en_vocab.pkl', 'wb')
# dump information to that file
pickle.dump(en_vocab, file)
file.close()
file = open('ja_vocab.pkl', 'wb')
pickle.dump(ja_vocab, file)
file.close()
```

Lastly, we can also save the model for later use using PyTorch save and load functions. Generally, there are two ways to save the model depending what we want to use them for later. The first one is for inference only, we can load the model later and use it to translate from Japanese to English.




```python
# save model for inference
torch.save(transformer.state_dict(), 'inference_model')
```

The second one is for inference too, but also for when we want to load the model later, and want to resume the training.




```python
# save model + checkpoint to resume training later
torch.save({
  'epoch': NUM_EPOCHS,
  'model_state_dict': transformer.state_dict(),
  'optimizer_state_dict': optimizer.state_dict(),
  'loss': train_loss,
  }, 'model_checkpoint.tar')
```

# Conclusion
Thatâ€™s it!

