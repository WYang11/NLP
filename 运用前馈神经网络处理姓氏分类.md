---
abbrlink: '0'
---

# 一、实验介绍

**1. 实验内容**

​	在本实验中，我们将研究两种前馈神经网络：多层感知器和卷积神经网络。多层感知器通过将多个感知器组合在单个层中，并叠加多个层来扩展简单感知器的结构。卷积神经网络受到窗口滤波器的启发，能够学习输入中的局部模式，使其在计算机视觉等领域中具有广泛应用。这两种神经网络都是前馈神经网络，与递归神经网络形成对比，后者允许反馈或循环，使得网络可以从之前的计算中获取信息。理解这些神经网络层对数据张量大小和形状的影响对于深入理解这些模型非常重要。

**2. 实验要点**

* 通过“示例:带有多层感知器的姓氏分类”，掌握多层感知器在多层分类中的应用
* 掌握每种类型的神经网络层对它所计算的数据张量的大小和形状的影响

**3. 实验环境**

* Python 3.6.7

# 二、The Multilayer Perceptron（多层感知器）

MLP（多层感知器）被认为是神经网络中最基本的构建模块之一。它是对感知器的扩展，将多个感知器组合在一起形成多个层，并引入非线性激活函数。在PyTorch中，通过设置线性层的输出特征数来构建MLP。一个简单的MLP通常包含三个阶段和两个线性层。第一个阶段是输入向量，代表给定给模型的数据。第一个线性层计算隐藏向量，称为第二阶段，因为它位于输入和输出之间。隐藏向量由多个感知器的输出组成。使用隐藏向量，第二个线性层计算输出向量。输出向量的大小取决于任务类型，可以是二元分类或多类分类的类别数。虽然本例中只展示了一个隐藏向量，但可以有多个中间阶段，每个阶段产生自己的隐藏向量。最终的隐藏向量通过线性层和非线性函数映射到输出向量。

![](https://yifdu.github.io/2018/12/20/Natural-Language-Processing-with-PyTorch%EF%BC%88%E5%9B%9B%EF%BC%89/MLP.png)

<font color='grey'><center>图1 一种具有两个线性层和三个表示阶段（输入向量、隐藏向量和输出向量)的MLP的可视化表示</font></center>

mlp的力量来自于添加第二个线性层和允许模型学习一个线性分割的的中间表示——该属性的能表示一个直线(或更一般的,一个超平面)可以用来区分数据点落在线(或超平面)的哪一边的。学习具有特定属性的中间表示，如分类任务是线性可分的，这是使用神经网络的最深刻后果之一，也是其建模能力的精髓。在下一节中，我们将更深入地研究这意味着什么。

**2.1 A Simple Example: XOR**

让我们看一下前面描述的XOR示例，看看感知器与MLP之间会发生什么。在这个例子中，我们在一个二元分类任务中训练感知器和MLP:星和圆。每个数据点是一个二维坐标。在不深入研究实现细节的情况下，最终的模型预测如图4-2所示。在这个图中，错误分类的数据点用黑色填充，而正确分类的数据点没有填充。在左边的面板中，从填充的形状可以看出，感知器在学习一个可以将星星和圆分开的决策边界方面有困难。然而，MLP(右面板)学习了一个更精确地对恒星和圆进行分类的决策边界。

![](https://yifdu.github.io/2018/12/20/Natural-Language-Processing-with-PyTorch%EF%BC%88%E5%9B%9B%EF%BC%89/MLP_1.png)
<font color='grey'><center>图2 从感知器(左)和MLP(右)学习的XOR问题的解决方案显示</font></center>

图2中，每个数据点的真正类是该点的形状:星形或圆形。错误的分类用块填充，正确的分类没有填充。这些线是每个模型的决策边界。在边的面板中，感知器学习—个不能正确地将圆与星分开的决策边界。事实上，没有一条线可以。在右动的面板中，MLP学会了从圆中分离星。

虽然在图中显示MLP有两个决策边界，这是它的优点，但它实际上只是一个决策边界!决策边界就是这样出现的，因为中间表示法改变了空间，使一个超平面同时出现在这两个位置上。在图4-4中，我们可以看到MLP计算的中间值。这些点的形状表示类(星形或圆形)。我们所看到的是，神经网络(本例中为MLP)已经学会了“扭曲”数据所处的空间，以便在数据通过最后一层时，用一线来分割它们。

![](https://yifdu.github.io/2018/12/20/Natural-Language-Processing-with-PyTorch%EF%BC%88%E5%9B%9B%EF%BC%89/MLP_2.png)
<font color='grey'><center>图4-3 MLP的输入和中间表示是可视化的。从左到右:（1）网络的输入;（2）第一个线性模块的输出;（3）第一个非线性模块的输出;（4)第二个线性模块的输出。第一个线性模块的输出将圆和星分组，而第二个线性模块的输出将数据点重新组织为线性可分的。</font></center>

相反，如图3所示，感知器没有额外的一层来处理数据的形状，直到数据变成线性可分的。

![](https://yifdu.github.io/2018/12/20/Natural-Language-Processing-with-PyTorch%EF%BC%88%E5%9B%9B%EF%BC%89/MLP_3.png)
<font color='grey'><center>图4 感知器的输入和输出表示。因为它没有像MLP那样的中间表示来分组和重新组织，所以它不能将圆和星分开。</font></center>


**2.2 Implementing MLPs in PyTorch**

在上一节中，我们概述了MLP的核心思想。在本节中，我们将介绍PyTorch中的一个实现。如前所述，MLP除了简单的感知器之外，还有一个额外的计算层。在我们在例2-1中给出的实现中，我们用PyTorch的两个线性模块实例化了这个想法。线性对象被命名为fc1和fc2，它们遵循一个通用约定，即将线性模块称为“完全连接层”，简称为“fc层”。除了这两个线性层外，还有一个修正的线性单元(ReLU)非线性，它在被输入到第二个线性层之前应用于第一个线性层的输出。由于层的顺序性，必须确保层中的输出数量等于下一层的输入数量。使用两个线性层之间的非线性是必要的，因为没有它，两个线性层在数学上等价于一个线性层4，因此不能建模复杂的模式。MLP的实现只实现反向传播的前向传递。这是因为PyTorch根据模型的定义和向前传递的实现，自动计算出如何进行向后传递和梯度更新。

Example 2-1. Multilayer Perceptron


```python
seed = 1337
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
```


```python
class MultilayerPerceptron(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        
        super(MultilayerPerceptron, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 第一个全连接层
        self.fc2 = nn.Linear(hidden_dim, output_dim)  # 第二个全连接层

    def forward(self, x_in, apply_softmax=False):
        
        
        intermediate = F.relu(self.fc1(x_in))  # 使用ReLU激活函数的第一个全连接层的输出
        output = self.fc2(intermediate)  # 第二个全连接层的输出
        
        if apply_softmax:
            output = F.softmax(output, dim=1)  # 应用softmax激活函数到输出层
        return output

```

在例2-2中，我们实例化了MLP。由于MLP实现的通用性，可以为任何大小的输入建模。为了演示，我们使用大小为3的输入维度、大小为4的输出维度和大小为100的隐藏维度。请注意，在print语句的输出中，每个层中的单元数很好地排列在一起，以便为维度3的输入生成维度4的输出。

Example 2-2. An example instantiation of an MLP


```python
# 定义输入批次大小、输入维度、隐藏层维度和输出维度
batch_size = 2  # 一次输入的样本数
input_dim = 3  # 输入向量的维度
hidden_dim = 100  # 第一个线性层（隐藏层）的输出大小
output_dim = 4  # 第二个线性层的输出大小

# 初始化模型
mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)
print(mlp)  # 打印MLP模型的架构

```

    MultilayerPerceptron(
      (fc1): Linear(in_features=3, out_features=100, bias=True)
      (fc2): Linear(in_features=100, out_features=4, bias=True)
    )


我们可以通过传递一些随机输入来快速测试模型的“连接”，如示例2-3所示。因为模型还没有经过训练，所以输出是随机的。在花费时间训练模型之前，这样做是一个有用的完整性检查。请注意PyTorch的交互性是如何让我们在开发过程中实时完成所有这些工作的，这与使用NumPy或panda没有太大区别:

Example 2-3. Testing the MLP with random inputs


```python
def describe(x):
    print("Type: {}".format(x.type()))  # 打印张量的数据类型
    print("Shape/size: {}".format(x.shape))  # 打印张量的形状/大小
    print("Values: \n{}".format(x))  # 打印张量的值

```


```python
# Inputs
x_input = torch.rand(batch_size, input_dim)
describe(x_input)
```

    Type: torch.FloatTensor
    Shape/size: torch.Size([2, 3])
    Values: 
    tensor([[0.8329, 0.4277, 0.4363],
            [0.9686, 0.6316, 0.8494]])



```python
y_output = mlp(x_input, apply_softmax=False)
describe(y_output)
```

    Type: torch.FloatTensor
    Shape/size: torch.Size([2, 4])
    Values: 
    tensor([[-0.2456,  0.0723,  0.1589, -0.3294],
            [-0.3497,  0.0828,  0.3391, -0.4271]], grad_fn=<AddmmBackward>)


__上述代码运行结果：__
```
Type: torch.FloatTensor
Shape/size: torch.Size([2, 3])
Values: 
tensor([[0.6193, 0.7045, 0.7812],
        [0.6345, 0.4476, 0.9909]])
```

学习如何读取PyTorch模型的输入和输出非常重要。在前面的例子中，MLP模型的输出是一个有两行四列的张量。这个张量中的行与批处理维数对应，批处理维数是小批处理中的数据点的数量。列是每个数据点的最终特征向量。在某些情况下，例如在分类设置中，特征向量是一个预测向量。名称为“预测向量”表示它对应于一个概率分布。预测向量会发生什么取决于我们当前是在进行训练还是在执行推理。在训练期间，输出按原样使用，带有一个损失函数和目标类标签的表示。我们将在“示例:带有多层感知器的姓氏分类”中对此进行深入介绍。

但是，如果想将预测向量转换为概率，则需要额外的步骤。具体来说，需要softmax函数，它用于将一个值向量转换为概率。softmax有许多根。在物理学中，它被称为玻尔兹曼或吉布斯分布;在统计学中，它是多项式逻辑回归;在自然语言处理(NLP)社区，它是最大熵(MaxEnt)分类器。不管叫什么名字，这个函数背后的直觉是，大的正值会导致更高的概率，小的负值会导致更小的概率。在示例2-3中，apply_softmax参数应用了这个额外的步骤。在例2-4中，可以看到相同的输出，但是这次将apply_softmax标志设置为True:

Example 2-4. MLP with apply_softmax=True


```python
y_output = mlp(x_input, apply_softmax=True)
describe(y_output)
```

    Type: torch.FloatTensor
    Shape/size: torch.Size([2, 4])
    Values: 
    tensor([[0.2087, 0.2868, 0.3127, 0.1919],
            [0.1832, 0.2824, 0.3649, 0.1696]], grad_fn=<SoftmaxBackward>)


**上述代码运行结果：**
```
Type: torch.FloatTensor
Shape/size: torch.Size([2, 4])
Values: 
tensor([[0.2915, 0.2541, 0.2277, 0.2267],
        [0.2740, 0.2735, 0.2189, 0.2336]], grad_fn=<SoftmaxBackward>)
```

综上所述，mlp是将张量映射到其他张量的线性层。在每一对线性层之间使用非线性来打破线性关系，并允许模型扭曲向量空间。在分类设置中，这种扭曲应该导致类之间的线性可分性。另外，可以使用softmax函数将MLP输出解释为概率，但是不应该将softmax与特定的损失函数一起使用，因为底层实现可以利用高级数学/计算捷径。

# 三、实验步骤

**3.1 Example: Surname Classification with a Multilayer Perceptron**

在本节中，我们将MLP用于姓氏分类任务，通过从数据中推断人口统计信息，如国籍。这些人口统计信息称为“受保护属性”，在使用时需要谨慎处理。

首先，我们将姓氏拆分为字符，并处理这些字符。字符层模型与基于单词的模型在结构和实现上相似。

MLP的实现和训练直接从第3章的感知器发展而来。我们不包括“餐馆评论情绪分类”中的所有代码，但基础相同。

本节内容包括：

1. 描述姓氏数据集及其预处理步骤。
2. 使用词汇表、向量化器和DataLoader类将姓氏字符串转化为小批处理向量。
3. 描述姓氏分类器模型及其设计过程。
4. 训练例程，与“餐馆评论情绪分类”中的方法相似。

通过这些步骤，我们展示了如何将MLP应用于多类分类任务。

**3.1.1 The Surname Dataset**

姓氏数据集包含来自18个不同国家的10,000个姓氏，这些姓氏由作者从互联网上不同的姓名来源收集。这个数据集在多个实验中重用，具有一些有趣的属性：

1. **不平衡性**：数据集非常不平衡，排名前三的类别占60%以上：27%是英语，21%是俄语，14%是阿拉伯语。剩下的15个民族频率逐渐减少，这是语言特有的特性。
2. **拼写与国籍的关系**：国籍和姓氏拼写之间有有效且直观的关系。例如，“O 'Neill”、“Antonopoulos”、“Nagasawa”或“Zhu”都与特定国籍紧密关联。

为了创建最终的数据集，我们从一个较少处理的版本开始，并进行了一些修改。首先，我们通过选择标记为俄语的姓氏的随机子集来减少不平衡性，因为原始数据集中70%以上是俄文。然后，我们根据国籍对数据集进行分组，并将数据集分为训练集（70%）、验证集（15%）和测试集（15%），以确保类标签在各部分间的分布相似。

Example 3-1. Implementing SurnameDataset.`__getitem__()`


```python
class SurnameDataset(Dataset):
    def __init__(self, surname_df, vectorizer):
        """
        初始化数据集
        
        参数:
            surname_df (pandas.DataFrame): 数据集
            vectorizer (SurnameVectorizer): 从数据集中实例化的向量化器
        """
        self.surname_df = surname_df
        self._vectorizer = vectorizer

        # 拆分数据集为训练、验证和测试集
        self.train_df = self.surname_df[self.surname_df.split == 'train']
        self.train_size = len(self.train_df)

        self.val_df = self.surname_df[self.surname_df.split == 'val']
        self.validation_size = len(self.val_df)

        self.test_df = self.surname_df[self.surname_df.split == 'test']
        self.test_size = len(self.test_df)

        self._lookup_dict = {'train': (self.train_df, self.train_size),
                             'val': (self.val_df, self.validation_size),
                             'test': (self.test_df, self.test_size)}

        self.set_split('train')
        
        # 计算类别权重
        class_counts = surname_df.nationality.value_counts().to_dict()
        def sort_key(item):
            return self._vectorizer.nationality_vocab.lookup_token(item[0])
        sorted_counts = sorted(class_counts.items(), key=sort_key)
        frequencies = [count for _, count in sorted_counts]
        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)

    @classmethod
    def load_dataset_and_make_vectorizer(cls, surname_csv):
        """加载数据集并从头创建新的向量化器
        
        参数:
            surname_csv (str): 数据集的位置
        返回:
            SurnameDataset 的实例
        """
        surname_df = pd.read_csv(surname_csv)
        train_surname_df = surname_df[surname_df.split == 'train']
        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))

    @classmethod
    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):
        """加载数据集和相应的向量化器。用于向量化器已缓存以便重用的情况
        
        参数:
            surname_csv (str): 数据集的位置
            vectorizer_filepath (str): 已保存的向量化器的位置
        返回:
            SurnameDataset 的实例
        """
        surname_df = pd.read_csv(surname_csv)
        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)
        return cls(surname_df, vectorizer)

    @staticmethod
    def load_vectorizer_only(vectorizer_filepath):
        """从文件加载向量化器的静态方法
        
        参数:
            vectorizer_filepath (str): 序列化向量化器的位置
        返回:
            SurnameVectorizer 的实例
        """
        with open(vectorizer_filepath) as fp:
            return SurnameVectorizer.from_serializable(json.load(fp))

    def save_vectorizer(self, vectorizer_filepath):
        """使用 json 将向量化器保存到磁盘
        
        参数:
            vectorizer_filepath (str): 保存向量化器的位置
        """
        with open(vectorizer_filepath, "w") as fp:
            json.dump(self._vectorizer.to_serializable(), fp)

    def get_vectorizer(self):
        """返回向量化器"""
        return self._vectorizer

    def set_split(self, split="train"):
        """选择数据集的拆分
        
        参数:
            split (str): 数据集的拆分 ('train', 'val', 'test')
        """
        self._target_split = split
        self._target_df, self._target_size = self._lookup_dict[split]

    def __len__(self):
        return self._target_size

    def __getitem__(self, index):
        """获取数据点
        
        参数:
            index (int): 数据点的索引
        返回:
            包含特征和标签的字典
        """
        row = self._target_df.iloc[index]

        surname_vector = self._vectorizer.vectorize(row.surname)
        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)

        return {'x_surname': surname_vector,
                'y_nationality': nationality_index}

    def get_num_batches(self, batch_size):
        """给定批次大小，返回数据集中的批次数量
        
        参数:
            batch_size (int)
        返回:
            数据集中的批次数量
        """
        return len(self) // batch_size

def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device="cpu"): 
    """
    一个包装 PyTorch DataLoader 的生成器函数。它将确保每个张量位于正确的设备位置。
    
    参数:
        dataset (Dataset): 数据集
        batch_size (int): 批次大小
        shuffle (bool): 是否随机打乱数据
        drop_last (bool): 是否丢弃最后一个不足批次大小的数据
        device (str): 设备 (cpu 或 cuda)
    """
    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)

    for data_dict in dataloader:
        out_data_dict = {}
        for name, tensor in data_dict.items():
            out_data_dict[name] = data_dict[name].to(device)
        yield out_data_dict

```

**3.1.2 Vocabulary, Vectorizer, and DataLoader**

为了将姓氏字符分类为国籍，我们使用词汇表、向量化器和DataLoader，将姓氏字符串转换为向量化的小批量数据。这些数据结构与“示例:分类餐馆评论的情感”中的数据结构相同。数据通过将字符映射到整数进行向量化，而不是将单词映射到整数。

### 词汇表类

在本例中使用的词汇表类与“示例:分类餐馆评论的情感”中的词汇表相同，将Yelp评论中的单词映射到相应的整数。简要概述如下，词汇表是两个Python字典的协调，这两个字典在令牌（在本例中是字符）和整数之间形成一个双向映射。主要方法有：

- `add_token`：向词汇表中添加新令牌
- `lookup_token`：检索字符对应的索引
- `lookup_index`：检索索引对应的字符

与Yelp评论的词汇表不同，我们使用的是one-hot词汇表，不计算字符出现的频率，只对频繁出现的条目进行限制。这主要是因为数据集较小。

### 姓氏向量化器

虽然词汇表将单个令牌（字符）转换为整数，但`SurnameVectorizer`负责应用词汇表并将姓氏转换为向量。其实例化和使用方式与“示例:分类餐馆评论的情感”中的`ReviewVectorizer`非常相似，但有一个关键区别：字符串没有在空格上分割。姓氏是字符的序列，每个字符在词汇表中是一个单独的标记。我们为以前未遇到的字符指定一个特殊的令牌，即`UNK`。由于我们仅从训练数据实例化词汇表，而且验证或测试数据中可能有唯一的字符，所以在字符词汇表中仍然使用`UNK`符号。

虽然在这个示例中我们使用了收缩的one-hot表示，但在后面的实验中，将了解其他向量化方法，这些方法有时是one-hot编码的替代方案，甚至更好。

以下是简化的代码示例：

Example 3-2. Implementing SurnameVectorizer


```python
class Vocabulary(object):

    def __init__(self, token_to_idx=None, add_unk=True, unk_token="<UNK>"):
   
        # 如果没有提供 token_to_idx 字典，则初始化一个空字典
        if token_to_idx is None:
            token_to_idx = {}
        self._token_to_idx = token_to_idx

        # 创建从索引到令牌的反向映射字典
        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}

        self._add_unk = add_unk
        self._unk_token = unk_token

        # 初始化 UNK 令牌的索引
        self.unk_index = -1
        if add_unk:
            self.unk_index = self.add_token(unk_token)

    def to_serializable(self):
        """Returns a dictionary that can be serialized."""
        return {'token_to_idx': self._token_to_idx, 
                'add_unk': self._add_unk, 
                'unk_token': self._unk_token}

    @classmethod
    def from_serializable(cls, contents):
        return cls(**contents)

    def add_token(self, token):
        # 如果令牌不在字典中，则添加它
        if token not in self._token_to_idx:
            index = len(self._token_to_idx)
            self._token_to_idx[token] = index
            self._idx_to_token[index] = token
        else:
            index = self._token_to_idx[token]
        return index

    def add_many(self, tokens):
        return [self.add_token(token) for token in tokens]

    def lookup_token(self, token):
        
        # 如果令牌不在字典中，则返回 UNK 令牌的索引
        if self.unk_index >= 0:
            return self._token_to_idx.get(token, self.unk_index)
        else:
            return self._token_to_idx[token]

    def lookup_index(self, index):

        # 如果索引不在字典中，则抛出 KeyError 异常
        if index not in self._idx_to_token:
            raise KeyError("the index (%d) is not in the Vocabulary" % index)
        return self._idx_to_token[index]

    def __str__(self):
        """Return a string representation of the Vocabulary."""
        return "<Vocabulary(size=%d)>" % len(self)

    def __len__(self):
        """Return the number of tokens in the Vocabulary."""
        return len(self._token_to_idx)

      
```


```python
class SurnameVectorizer(object):
    """The Vectorizer which coordinates the Vocabularies and puts them to use"""

    def __init__(self, surname_vocab, nationality_vocab):
   
        self.surname_vocab = surname_vocab
        self.nationality_vocab = nationality_vocab

    def vectorize(self, surname):
        
        vocab = self.surname_vocab
        one_hot = np.zeros(len(vocab), dtype=np.float32)
        for token in surname:
            one_hot[vocab.lookup_token(token)] = 1

        return one_hot

    @classmethod
    def from_dataframe(cls, surname_df):
      
        surname_vocab = Vocabulary(unk_token="@")
        nationality_vocab = Vocabulary(add_unk=False)

        # 构建姓氏和国籍的词汇表
        for index, row in surname_df.iterrows():
            for letter in row.surname:
                surname_vocab.add_token(letter)
            nationality_vocab.add_token(row.nationality)

        return cls(surname_vocab, nationality_vocab)

    @classmethod
    def from_serializable(cls, contents):
        
        # 从序列化的内容中恢复姓氏和国籍的词汇表
        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])
        nationality_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])
        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)

    def to_serializable(self):
        return {'surname_vocab': self.surname_vocab.to_serializable(),
                'nationality_vocab': self.nationality_vocab.to_serializable()}

```

**3.1.3 The Surname Classifier Model**

SurnameClassifier是一个MLP，由两个线性层组成，用于将输入向量映射到中间向量，然后映射到预测向量。可选择在最后一步应用softmax操作以确保输出是概率。交叉熵损失对于多类分类最理想，但在训练中软最大值的计算既浪费又不稳定。

**3.1.4 The Training Routine**

训练程序的一般操作序列包括以下步骤：实例化数据集、模型、损失函数和优化器，然后在训练分区上更新模型参数，评估性能并在验证分区上进行测量。这个过程会在数据集上重复多次，直到达到指定的迭代次数。虽然具体的实现可能会因输入参数而异，但其基本结构与先前示例中的训练例程相似。

Example 3-3. The args for classifying surnames with an MLP


```python
# 设置参数
args = Namespace(
    # 数据和路径信息
    surname_csv="data/surnames/surnames_with_splits.csv",
    vectorizer_file="vectorizer.json",
    model_state_file="model.pth",
    save_dir="model_storage/ch4/cnn",
    # 模型超参数
    hidden_dim=100,
    num_channels=256,
    # 训练超参数
    seed=1337,
    learning_rate=0.001,
    batch_size=128,
    num_epochs=100,
    early_stopping_criteria=5,
    dropout_p=0.1,
    # 运行时选项
    cuda=False,
    reload_from_files=False,
    expand_filepaths_to_save_dir=True,
    catch_keyboard_interrupt=True
)
 
# 扩展文件路径
if args.expand_filepaths_to_save_dir:
    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)
    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)
    
    print("扩展的文件路径: ")
    print("\t{}".format(args.vectorizer_file))
    print("\t{}".format(args.model_state_file))
    
# 检查 CUDA 可用性
if not torch.cuda.is_available():
    args.cuda = False
 
args.device = torch.device("cuda" if args.cuda else "cpu")
print("使用 CUDA: {}".format(args.cuda))
 
def set_seed_everywhere(seed, cuda):
    """
    设置随机种子以确保结果的可重复性
    Args:
        seed (int): 随机种子
        cuda (bool): 是否使用 CUDA
    """
    np.random.seed(seed)
    torch.manual_seed(seed)
    if cuda:
        torch.cuda.manual_seed_all(seed)
        
def handle_dirs(dirpath):
    """
    确保目录存在
    Args:
        dirpath (str): 目录路径
    """
    if not os.path.exists(dirpath):
        os.makedirs(dirpath)
        
# 设置随机种子以确保可重复性
set_seed_everywhere(args.seed, args.cuda)
 
# 处理目录
handle_dirs(args.save_dir)


```


**3.1.5 Model Evaluation and Prediction**

要理解模型的性能，应该使用定量和定性方法分析模型。定量测量出的测试数据的误差，决定了分类器能否推广到不可见的例子。定性地说，可以通过查看分类器的top-k预测来为一个新示例开发模型所了解的内容的直觉。

示例3-4显示了分类新姓氏的代码。给定一个姓氏作为字符串，该函数将首先应用向量化过程，然后获得模型预测。注意，我们包含了apply_softmax标志，所以结果包含概率。模型预测，在多项式的情况下，是类概率的列表。我们使用PyTorch张量最大函数来得到由最高预测概率表示的最优类。

Example 3-4. A function for performing nationality prediction


```python
# 加载最佳模型的状态字典
classifier.load_state_dict(torch.load(train_state['model_filename']))

# 将模型移到相应的设备上
classifier = classifier.to(args.device)

# 使用类权重设置损失函数
loss_func = nn.CrossEntropyLoss(dataset.class_weights)

# 将数据划分设置为'test'，并初始化批次生成器
dataset.set_split('test')
batch_generator = generate_batches(dataset, 
                                   batch_size=args.batch_size, 
                                   device=args.device)

# 初始化用于累积损失和准确率的变量
running_loss = 0.
running_acc = 0.

# 将模型设置为评估模式
classifier.eval()

# 遍历测试集中的批次
for batch_index, batch_dict in enumerate(batch_generator):
    # 计算输出预测
    y_pred = classifier(batch_dict['x_surname'])
    
    # 计算损失
    loss = loss_func(y_pred, batch_dict['y_nationality'])
    loss_t = loss.item()
    # 更新累积损失
    running_loss += (loss_t - running_loss) / (batch_index + 1)

    # 计算准确率
    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])
    # 更新累积准确率
    running_acc += (acc_t - running_acc) / (batch_index + 1)

# 将计算得到的测试损失和准确率存储在train_state字典中
train_state['test_loss'] = running_loss
train_state['test_acc'] = running_acc

```


```python
print("Test loss: {};".format(train_state['test_loss']))
print("Test Accuracy: {}".format(train_state['test_acc']))
```

    Test loss: 1.8689435815811157;
    Test Accuracy: 47.062500000000014



```python
def predict_nationality(surname, classifier, vectorizer):
    """
    预测一个新姓氏的国籍
    
    Args:
        surname (str): 待分类的姓氏
        classifier (SurnameClassifer): 分类器的实例
        vectorizer (SurnameVectorizer): 对应的向量化器
    Returns:
        包含最可能的国籍及其概率的字典
    """
    # 将姓氏向量化
    vectorized_surname = vectorizer.vectorize(surname)
    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)
    
    # 使用分类器进行预测，并应用softmax操作
    result = classifier(vectorized_surname, apply_softmax=True)

    # 获取最大概率值和对应的索引
    probability_values, indices = result.max(dim=1)
    index = indices.item()

    # 获取预测的国籍及其概率值
    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)
    probability_value = probability_values.item()

    return {'nationality': predicted_nationality, 'probability': probability_value}

```

**3.1.5.3 RETRIEVING THE TOP-K PREDICTIONS FOR A NEW SURNAME**

不仅要看最好的预测，还要看更多的预测。例如，NLP中的标准实践是采用k-best预测并使用另一个模型对它们重新排序。PyTorch提供了一个torch.topk函数，它提供了一种方便的方法来获得这些预测，如示例3-5所示。

Example 3-5. Predicting the top-k nationalities


```python
vectorizer.nationality_vocab.lookup_index(8)
```


    'Irish'


```python
def predict_topk_nationality(name, classifier, vectorizer, k=5):
    """
    预测一个姓氏的前k个可能的国籍
    
    Args:
        name (str): 待分类的姓名
        classifier (SurnameClassifer): 分类器的实例
        vectorizer (SurnameVectorizer): 对应的向量化器
        k (int): 返回前k个预测结果，默认为5
    Returns:
        包含前k个最可能国籍及其概率的列表
    """
    # 将姓名向量化
    vectorized_name = vectorizer.vectorize(name)
    vectorized_name = torch.tensor(vectorized_name).view(1, -1)
    
    # 使用分类器进行预测，并应用softmax操作
    prediction_vector = classifier(vectorized_name, apply_softmax=True)
    probability_values, indices = torch.topk(prediction_vector, k=k)
    
    # 将张量转换为NumPy数组，返回的大小为1*k
    probability_values = probability_values.detach().numpy()[0]
    indices = indices.detach().numpy()[0]
    
    results = []
    for prob_value, index in zip(probability_values, indices):
        # 获取预测的国籍及其概率值
        nationality = vectorizer.nationality_vocab.lookup_index(index)
        results.append({'nationality': nationality, 
                        'probability': prob_value})
    
    return results

# 获取用户输入的新姓氏
new_surname = input("请输入一个姓氏进行分类: ")
classifier = classifier.to("cpu")

# 获取用户想要查看的前k个预测结果数量
k = int(input("想要查看前几个预测结果? "))
if k > len(vectorizer.nationality_vocab):
    print("抱歉！这比我们拥有的国籍数量还要多.. 默认您查看全部结果 :)")
    k = len(vectorizer.nationality_vocab)
    
# 进行预测
predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)

# 打印预测结果
print("前{}个预测结果:".format(k))
print("===================")
for prediction in predictions:
    print("{} -> {} (概率={:0.2f})".format(new_surname,
                                        prediction['nationality'],
                                        prediction['probability']))

```

    Enter a surname to classify:  wang
    How many of the top predictions to see?  5


    Top 5 predictions:
    ===================
    wang -> Chinese (p=0.53)
    wang -> Korean (p=0.27)
    wang -> English (p=0.11)
    wang -> Japanese (p=0.02)
    wang -> Polish (p=0.02)



**3.4 Example: Classifying Surnames by Using a CNN**

在本实验的第一部分中，我们深入研究了MLPs、由一系列线性层和非线性函数构建的神经网络。

在本节中，我们将介绍卷积神经网络(CNN)，这是一种非常适合检测空间子结构(并因此创建有意义的空间子结构)的神经网络。CNNs通过使用少量的权重来扫描输入数据张量来实现这一点。通过这种扫描，它们产生表示子结构检测(或不检测)的输出张量。

为了证明CNN的有效性，让我们应用一个简单的CNN模型来分类姓氏。这项任务的许多细节与前面的MLP示例相同，但真正发生变化的是模型的构造和向量化过程。模型的输入，而不是我们在上一个例子中看到的收缩的onehot，将是一个onehot的矩阵。这种设计将使CNN能够更好地“view”字符的排列，并对在“示例:带有多层感知器的姓氏分类”中使用的收缩的onehot编码中丢失的序列信息进行编码。

**3.4.1 The SurnameDataset**

虽然姓氏数据集之前在“示例:带有多层感知器的姓氏分类”中进行了描述，但建议参考“姓氏数据集”来了解它的描述。尽管我们使用了来自“示例:带有多层感知器的姓氏分类”中的相同数据集，但在实现上有一个不同之处:数据集由onehot向量矩阵组成，而不是一个收缩的onehot向量。为此，我们实现了一个数据集类，它跟踪最长的姓氏，并将其作为矩阵中包含的行数提供给矢量化器。列的数量是onehot向量的大小(词汇表的大小)。示例3-6显示了对`SurnameDataset.__getitem__`的更改;我们显示对SurnameVectorizer的更改。在下一小节向量化。

我们使用数据集中最长的姓氏来控制onehot矩阵的大小有两个原因。首先，将每一小批姓氏矩阵组合成一个三维张量，要求它们的大小相同。其次，使用数据集中最长的姓氏意味着可以以相同的方式处理每个小批处理。

Example 3-6. SurnameDataset modified for passing the maximum surname length


```python
def __getitem__(self, index):
    """
    PyTorch数据集的主要入口方法
    
    Args:
        index (int): 数据点的索引
    Returns:
        包含数据点特征和标签的字典:
            特征 (x_surname)
            标签 (y_nationality)
    """
    # 获取指定索引的数据行
    row = self._target_df.iloc[index]

    # 将姓氏向量化
    surname_vector = self._vectorizer.vectorize(row.surname)

    # 获取国籍的索引
    nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)

    return {'x_surname': surname_vector,
            'y_nationality': nationality_index}

```

**3.4.2 Vocabulary, Vectorizer, and DataLoader**

在本例中，尽管词汇表和DataLoader的实现方式与“示例:带有多层感知器的姓氏分类”中的示例相同，但Vectorizer的vectorize()方法已经更改，以适应CNN模型的需要。具体来说，正如我们在示例4-18中的代码中所示，该函数将字符串中的每个字符映射到一个整数，然后使用该整数构造一个由onehot向量组成的矩阵。重要的是，矩阵中的每一列都是不同的onehot向量。主要原因是，我们将使用的Conv1d层要求数据张量在第0维上具有批处理，在第1维上具有通道，在第2维上具有特性。

除了更改为使用onehot矩阵之外，我们还修改了矢量化器，以便计算姓氏的最大长度并将其保存为max_surname_length

Example 3-7. Implementing the Surname Vectorizer for CNNs


```python
class SurnameVectorizer(object):
    """ The Vectorizer which coordinates the Vocabularies and puts them to use"""
    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):
        """
        Args:
            surname_vocab (Vocabulary): 将字符映射到整数的词汇表
            nationality_vocab (Vocabulary): 将国籍映射到整数的词汇表
            max_surname_length (int): 最长姓氏的长度
        """
        self.surname_vocab = surname_vocab
        self.nationality_vocab = nationality_vocab
        self._max_surname_length = max_surname_length

    def vectorize(self, surname):
        """
        Args:
            surname (str): 姓氏
        Returns:
            one_hot_matrix (np.ndarray): 一个包含独热向量的矩阵
        """

        # 初始化一个矩阵，用于存储独热向量
        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)
        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)
                               
        # 对于每个字符，将其对应的独热向量位置设为1
        for position_index, character in enumerate(surname):
            character_index = self.surname_vocab.lookup_token(character)
            one_hot_matrix[character_index][position_index] = 1
        
        return one_hot_matrix

    @classmethod
    def from_dataframe(cls, surname_df):
        """从数据框实例化向量化器
        
        Args:
            surname_df (pandas.DataFrame): 姓氏数据集
        Returns:
            SurnameVectorizer的一个实例
        """
        # 初始化姓氏和国籍的词汇表，以及最长姓氏的长度
        surname_vocab = Vocabulary(unk_token="@")
        nationality_vocab = Vocabulary(add_unk=False)
        max_surname_length = 0

        # 遍历数据集，更新词汇表和最长姓氏的长度
        for index, row in surname_df.iterrows():
            max_surname_length = max(max_surname_length, len(row.surname))
            for letter in row.surname:
                surname_vocab.add_token(letter)
            nationality_vocab.add_token(row.nationality)

        return cls(surname_vocab, nationality_vocab, max_surname_length)

    @classmethod
    def from_serializable(cls, contents):
        # 从序列化内容中实例化SurnameVectorizer
        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])
        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])
        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, 
                   max_surname_length=contents['max_surname_length'])

    def to_serializable(self):
        # 将SurnameVectorizer序列化为字典
        return {'surname_vocab': self.surname_vocab.to_serializable(),
                'nationality_vocab': self.nationality_vocab.to_serializable(), 
                'max_surname_length': self._max_surname_length}

```

**3.4.3 Reimplementing the SurnameClassifier with Convolutional Networks**

我们在本例中使用的模型是使用我们在“卷积神经网络”中介绍的方法构建的。实际上，我们在该部分中创建的用于测试卷积层的“人工”数据与姓氏数据集中使用本例中的矢量化器的数据张量的大小完全匹配。正如在示例3-8中所看到的，它与我们在“卷积神经网络”中引入的Conv1d序列既有相似之处，也有需要解释的新添加内容。具体来说，该模型类似于“卷积神经网络”，它使用一系列一维卷积来增量地计算更多的特征，从而得到一个单特征向量。

然而，本例中的新内容是使用sequence和ELU PyTorch模块。序列模块是封装线性操作序列的方便包装器。在这种情况下，我们使用它来封装Conv1d序列的应用程序。ELU是类似于的ReLU的非线性函数，但是它不是将值裁剪到0以下，而是对它们求幂。ELU已经被证明是卷积层之间使用的一种很有前途的非线性(Clevert et al.， 2015)。

在本例中，我们将每个卷积的通道数与num_channels超参数绑定。我们可以选择不同数量的通道分别进行卷积运算。这样做需要优化更多的超参数。我们发现256足够大，可以使模型达到合理的性能。

由于后续训练和预测部分都相似，篇幅限制，在此不做展开。

Example 3-8. The CNN-based SurnameClassifier


```python
class SurnameClassifier(nn.Module):
    def __init__(self, initial_num_channels, num_classes, num_channels):
        """
        Args:
            initial_num_channels (int): 输入特征向量的大小
            num_classes (int): 输出预测向量的大小
            num_channels (int): 网络中要使用的恒定通道大小
        """
        super(SurnameClassifier, self).__init__()
        
        # 定义卷积神经网络层
        self.convnet = nn.Sequential(
            nn.Conv1d(in_channels=initial_num_channels, 
                      out_channels=num_channels, kernel_size=3),
            nn.ELU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3, stride=2),
            nn.ELU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3, stride=2),
            nn.ELU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3),
            nn.ELU()
        )
        # 定义全连接层
        self.fc = nn.Linear(num_channels, num_classes)

    def forward(self, x_surname, apply_softmax=False):
        """分类器的前向传播
        
        Args:
            x_surname (torch.Tensor): 输入数据张量。 
                x_surname.shape 应为 (batch, initial_num_channels, max_surname_length)
            apply_softmax (bool): softmax激活的标志
                如果与交叉熵损失一起使用，则应为false
        Returns:
            结果张量。tensor.shape 应为 (batch, num_classes)
        """
        # 运行卷积层
        features = self.convnet(x_surname).squeeze(dim=2)
       
        # 运行全连接层
        prediction_vector = self.fc(features)

        if apply_softmax:
            # 应用softmax激活函数
            prediction_vector = F.softmax(prediction_vector, dim=1)

        return prediction_vector

```


